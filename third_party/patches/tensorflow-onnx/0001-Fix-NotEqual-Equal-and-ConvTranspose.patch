From 3dd304c8d8d117c93c0d39ddc5b1e9eecdd9a662 Mon Sep 17 00:00:00 2001
From: "Yuchi.Wen" <Yuchi.Wen@sony.com>
Date: Thu, 25 Jul 2019 13:51:37 +0800
Subject: [PATCH] Fix NotEqual Equal and ConvTranspose

---
 tf2onnx/onnx_opset/logical.py | 47 +++++++++++++++++------------------
 tf2onnx/onnx_opset/nn.py      | 10 +++++---
 2 files changed, 30 insertions(+), 27 deletions(-)

diff --git a/tf2onnx/onnx_opset/logical.py b/tf2onnx/onnx_opset/logical.py
index 20f768f..788b291 100644
--- a/tf2onnx/onnx_opset/logical.py
+++ b/tf2onnx/onnx_opset/logical.py
@@ -47,37 +47,36 @@ class BroadcastOp(common.BroadcastOp):
     pass
 
 
-@tf_op(["Equal", "NotEqual"])
+@tf_op("Equal")
 class Equal:
     @classmethod
     def version_1(cls, ctx, node, **kwargs):
-        need_not = node.type == "NotEqual"
         common.BroadcastOp.version_1(ctx, node, **kwargs)
-        if need_not:
-            node.type = "Equal"
-            output_name = node.output[0]
-            not_node = ctx.insert_new_node_on_output("Not", output_name, name=utils.make_name(node.name))
-            ctx.copy_shape(output_name, not_node.output[0])
-            ctx.copy_dtype(output_name, not_node.output[0])
 
     @classmethod
     def version_7(cls, ctx, node, **kwargs):
-        # T2 output = Equal(T1, x, T1 y), T1 \in {bool, int32, int64}
-        need_not = node.type == "NotEqual"
-        supported_dtypes = [
-            TensorProto.BOOL,
-            TensorProto.INT32,
-            TensorProto.INT64
-        ]
-        # FIXME: casting is not the same as equal
-        target_dtype = TensorProto.INT32
-        _add_cast_to_inputs(ctx, node, supported_dtypes, target_dtype)
-        if need_not:
-            node.type = "Equal"
-            output_name = node.output[0]
-            not_node = ctx.insert_new_node_on_output("Not", output_name, name=utils.make_name(node.name))
-            ctx.copy_shape(output_name, not_node.output[0])
-            ctx.copy_dtype(output_name, not_node.output[0])
+        pass
+
+
+@tf_op("NotEqual")
+class NotEqual:
+    @classmethod
+    def version_1(cls, ctx, node, **kwargs):
+        common.BoradcastOp.version_1(ctx, node, **kwargs)
+        node.type = "Equal"
+        output_name = node.output[0]
+        not_node = ctx.insert_new_node_on_output("Not", output_name, name=utils.make_name(node.name))
+        ctx.copy_shape(output_name, not_node.output[0])
+        ctx.copy_dtype(output_name, not_node.output[0])
+
+    @classmethod
+    def version_7(cls, ctx, node, **kwargs):
+        node.type = "Equal"
+        output_name = node.output[0]
+        not_node = ctx.insert_new_node_on_output("Not", output_name, name=utils.make_name(node.name))
+        ctx.copy_shape(output_name, not_node.output[0])
+        ctx.copy_dtype(output_name, not_node.output[0])
+
 
 
 @tf_op(["Greater", "Less"])
diff --git a/tf2onnx/onnx_opset/nn.py b/tf2onnx/onnx_opset/nn.py
index 7003de6..ddc11ed 100644
--- a/tf2onnx/onnx_opset/nn.py
+++ b/tf2onnx/onnx_opset/nn.py
@@ -125,7 +125,7 @@ def conv_convert_inputs(ctx, node, with_kernel=False, new_kernel_shape=None,
         node.data_format = "NCHW"
 
 
-def add_padding(ctx, node, kernel_shape, strides, dilations=None, spatial=2):
+def add_padding(ctx, node, kernel_shape, strides, dilations=None, spatial=2, is_conv2d_transpose=False):
     padding = node.get_attr("padding")
     if padding:
         if dilations is None:
@@ -151,7 +151,10 @@ def add_padding(ctx, node, kernel_shape, strides, dilations=None, spatial=2):
                 node.set_attr("auto_pad", "SAME_UPPER")
             else:
                 for i in range(spatial):
-                    pad = (output_shape[i + 2] - 1) * strides[i] + dilations[i] * kernel_shape[i] - input_shape[i + 2]
+                    if is_conv2d_transpose:
+                        pad = (input_shape[i + 2] - 1) * strides[i] + dilations[i] * kernel_shape[i] - output_shape[i + 2]
+                    else:
+                        pad = (output_shape[i + 2] - 1) * strides[i] + dilations[i] * kernel_shape[i] - input_shape[i + 2]
                     pad = max(pad, 0)
                     pads[i] = pad // 2
                     pads[i + spatial] = pad - pad // 2
@@ -238,7 +241,7 @@ class ConvTranspose:
         node.set_attr("output_shape", new_output_shape)
 
         strides = conv_dims_attr(node, "strides")
-        conv_dims_attr(node, "dilations")
+        dilations = conv_dims_attr(node, "dilations")
 
         # remove output_shapes input
         ctx.remove_input(node, node.input[0])
@@ -247,6 +250,7 @@ class ConvTranspose:
         node.input[0] = node.input[1]
         node.input[1] = t
 
+        add_padding(ctx, node, kernel_shape, strides, dilations=dilations, spatial=2, is_conv2d_transpose=True)
         conv_convert_inputs(ctx, node, with_kernel=True)
 
 
-- 
2.17.1

