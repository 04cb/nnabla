# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2017, Sony Corporation
# This file is distributed under the same license as the Neural Network
# Libraries package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2020.
#
msgid ""
msgstr ""
"Project-Id-Version: Neural Network Libraries 1.7.0.dev1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-06-10 15:10+0900\n"
"PO-Revision-Date: 2020-06-11 15:58+0900\n"
"Last-Translator: \n"
"Language: ja_JP\n"
"Language-Team: \n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"
"X-Generator: Poedit 2.3.1\n"

#: ../../python/tutorial/mixed_precision_training.rst:3
msgid "Mixed Precision Training"
msgstr "Mixed Precision を用いた学習"

#: ../../python/tutorial/mixed_precision_training.rst:6
msgid "Introduction"
msgstr "はじめに"

#: ../../python/tutorial/mixed_precision_training.rst:8
msgid ""
"Traditionally, for training a neural network, we used to use ``FP32`` for "
"weights and activations; however computation costs for training a neural "
"network rapidly increase over years as the success of deep learning and "
"the growing size of a neural network. It indicates that we need to spend "
"much more time for training a huge size of a neural network while we "
"would like to do lots of trials before a product launch. To address this "
"problem, companies (e.g., NVIDIA) introduced an accelerator for speeding "
"up computation. For example, NVIDIA Volta has `Tensor Cores <https://"
"devblogs.nvidia.com/programming-tensor-cores-cuda-9/>`__ to speed up "
"computation."
msgstr ""
"従来、ニューラルネットワークの学習の際には重みと活性値として ``FP32`` "
"( 32bit 浮動小数点数 ) が用いられてきました。しかし、深層学習の成功と"
"ニューラルネットワークのサイズ増加に伴い、近年ニューラルネットワークの学習"
"にかかる計算コストが急速に増えています。これは、製品の発売前に多くの試行錯"
"誤を行うにあたり、巨大なサイズのニューラルネットワークの学習のためにより多"
"くの時間が必要になっていることを示しています。この問題に対処するために、深"
"層学習のためのハードウェアを提供する企業 ( 例 NVIDIA ) は計算を高速化する"
"ためのアクセラレーターを導入しました。例えば、NVIDIA の Volta 世代以降の "
"GPU は計算を高速化するために `Tensor Cores <https://devblogs.nvidia.com/"
"programming-tensor-cores-cuda-9/>`__ を備えています。"

#: ../../python/tutorial/mixed_precision_training.rst:20
msgid ""
"However, it uses ``FP16`` weights, activations, gradients, and the range "
"of ``FP16`` is very limited when compared to that of ``FP32``, meaning "
"that sometimes (or often) values of gradients overflow and/or underflow, "
"which affects the performance of a neural network or makes it collapse "
"during training."
msgstr ""
"しかし、 ``FP16`` ( 16bit 浮動小数点数 ) を重み、活性値、勾配に使うにあた"
"り、 ``FP16`` の表現力は ``FP32`` と比較して大きく制限されています。つま"
"り、 勾配の値のオーバーフローやアンダーフローが時々 ( あるいは、頻繁に ) "
"発生し、これによりニューラルネットワークのパフォーマンスに悪影響を及ぼした"
"り、学習の失敗を引き起こします。"

#: ../../python/tutorial/mixed_precision_training.rst:26
msgid ""
"Mixed precision training is one of the algorithms to circumvent that "
"problem while maintaining the same results that we could obtain with "
"``FP32`` networks. It is well-described in `The Training with Mixed "
"Precision User Guide <https://docs.nvidia.com/deeplearning/sdk/mixed-"
"precision-training/index.html>`__ and `Mixed Precision Training <https://"
"arxiv.org/abs/1710.03740>`__."
msgstr ""
"Mixed Precision を用いた学習は、 ``FP32`` ネットワークで得られるものと同じ"
"結果を維持しながら問題を回避する方法の 1 つです。詳しくは、 `混合精度学習"
"のユーザーガイド <https://docs.nvidia.com/deeplearning/sdk/mixed-"
"precision-training/index.html>`__ と `混合精度学習 <https://arxiv.org/"
"abs/1710.03740>`__ に記載しています。"

#: ../../python/tutorial/mixed_precision_training.rst:33
msgid ""
"This tutorial explains how to do the mixed precision training in NNabla "
"step-by-step."
msgstr ""
"このチュートリアルでは、 NNabla における Mixed Precision を用いた学習方法"
"を段階的に説明します。"

#: ../../python/tutorial/mixed_precision_training.rst:37
msgid "Step-by-Step Instruction"
msgstr "段階毎の説明"

#: ../../python/tutorial/mixed_precision_training.rst:39
msgid "Basically, the mixed precision training are composed of three parts."
msgstr ""
"基本的には、 Mixed Precision を用いた学習は 次の 3 つの段階で構成されてい"
"ます。"

#: ../../python/tutorial/mixed_precision_training.rst:41
msgid "Use the accelerator for computation (here we assume Tensor Cores)"
msgstr ""
"計算のためのアクセラレーターの使用 ( ここでは Tensor Cores を仮定していま"
"す )"

#: ../../python/tutorial/mixed_precision_training.rst:42
msgid "Use loss scaling to prevent underflow"
msgstr "オーバーフローを防ぐためのロススケーリングの使用"

#: ../../python/tutorial/mixed_precision_training.rst:43
msgid "Use dynamic loss scaling to prevent overflow/underflow"
msgstr ""
"オーバーフロー / アンダーフローを防ぐための動的ロススケーリングの使用"

#: ../../python/tutorial/mixed_precision_training.rst:45
msgid "In NNabla, we can do the correspondences as follows."
msgstr "NNabla では、次のように対応できます。"

#: ../../python/tutorial/mixed_precision_training.rst:48
msgid "1. Use Tensor Cores"
msgstr "1. Tensor Cores の使用"

#: ../../python/tutorial/mixed_precision_training.rst:50
msgid "ctx = get_extension_context(\"cudnn\", type_config=\"half\")"
msgstr ""

#: ../../python/tutorial/mixed_precision_training.rst:55
msgid "2. Use loss scaling to prevent underflow"
msgstr "2. アンダーフローを防ぐためのロススケーリングの使用"

#: ../../python/tutorial/mixed_precision_training.rst:57
msgid ""
"loss_scale = 8\n"
"loss.backward(loss_scale)\n"
"solver.scale_grad(1. / loss_scale)  # do some gradient clipping, etc. "
"after this\n"
"solver.update()"
msgstr ""

#: ../../python/tutorial/mixed_precision_training.rst:65
msgid "3. Use dynamic loss scaling to prevent overflow/underflow"
msgstr ""
"3. オーバーフロー / アンダーフローを防ぐための動的ロススケーリングの使用"

#: ../../python/tutorial/mixed_precision_training.rst:67
msgid ""
"loss_scale = 8\n"
"scaling_factor = 2\n"
"counter = 0\n"
"interval = 2000\n"
"...\n"
"loss.backward(loss_scale, ...)\n"
"...\n"
"if solver.check_inf_or_nan_grad():\n"
"    loss_scale /= scaling_factor\n"
"    counter = 0\n"
"else:\n"
"    solver.scale_grad(1. / loss_scale) # do some gradient clipping, etc. "
"after this\n"
"    solver.update()\n"
"    if counter > interval:\n"
"        loss_scale *= scaling_factor\n"
"        counter = 0\n"
"    counter += 1"
msgstr ""

#: ../../python/tutorial/mixed_precision_training.rst:87
msgid ""
"**Note** that currently the procedures of 2nd (Use loss scaling to "
"prevent underflow) and 3rd (Use loss scaling to prevent overflow) are "
"experimental, and we are now trying to speed up the mixed precision "
"training, so API might change for future use, especially 3rd."
msgstr ""
"**注** : 手順の 2 番目 ( アンダーフローを防ぐためのロス・スケーリングの使"
"用 ) と 3 番目 ( オーバーフローを防ぐためのロス・スケーリングの使用 ) は、"
"現在実験段階であり、混合精度学習の高速化に尽力しています。そのため、 "
"API 、特に 3 番目は、将来の使用のため変わる可能性があります。"

#: ../../python/tutorial/mixed_precision_training.rst:93
msgid "All-in-one Instruction"
msgstr "全ての処理を含んだ説明"

#: ../../python/tutorial/mixed_precision_training.rst:95
msgid ""
"In the previous step-by-step example, the 3rd step is lengthy in a "
"training loop, thus we can write a wrapper class like the following."
msgstr ""
"前述の段階的な説明では、学習ループ内における 3 番目のステップが非常に長く"
"なっています。代わりにここは次のようなラッパークラスを書くことができます。"

#: ../../python/tutorial/mixed_precision_training.rst:98
msgid ""
"class DynamicLossScalingUpdater(object):\n"
"    '''Dynamic Loss Scaling Updater for the mixed precision training.\n"
"\n"
"    Args:\n"
"        solver (:obj:`nnabla.solvers.Solver`): Solver object. E.g., "
"Momentum or Adam.\n"
"        loss (:obj:`nnabla.Variable`): Loss variable from which the "
"forward and the backward is called.\n"
"        data_feeder (callable :obj:`object`, function, or lambda): Data "
"feeder\n"
"        scale (:obj:`float`): Loss scale constant. This is dynamically "
"changing during training.\n"
"        scaling_factor (:obj:`float`): Scaling factor for the dynamic "
"loss scaling.\n"
"        N (:obj:`int`): Interval, the number of iterations in training "
"for increasing `loss scale` by `scaling_factor`.\n"
"        clear_buffer (:obj:`bool`): Clears the no longer referenced "
"variables during backpropagation to save memory.\n"
"        accum_grad (:obj:`int`): Number of accumulation of gradients. "
"Update method of the `solver` is called after the `accum_grad` number of "
"the forward and backward is called.\n"
"        weight_decay (:obj:`float`): Decay constant. Default is `None`, "
"not applying the weight decay.\n"
"        comm (:obj:`nnabla.communicators.Communicator`): Communicator "
"when to do distributed training. Default is :obj:`None`.\n"
"        grads (:obj:`list` of :obj:`nnabla._nd_array.NdArray`): The list "
"of gradients to be exchanged when to do distributed training. Default is "
"the empty :obj:`list`.\n"
"\n"
"    Attributes:\n"
"        solver (:obj:`nnabla.solvers.Solver`): Solver object. E.g., "
"Momentum or Adam.\n"
"        loss (:obj:`nnabla.Variable`): Loss variable from which the "
"forward and the backward is called.\n"
"        data_feeder (callable :obj:`object`, function, lambda): Data "
"feeder\n"
"        scale (:obj:`float`): Loss scale constant. This is dynamically "
"changing during training.\n"
"        scaling_factor (:obj:`float`): Scaling factor for the dynamic "
"loss scaling.\n"
"        N (:obj:`int`): Interval, the number of iterations in training "
"for increasing `loss scale` by `scaling_factor`.\n"
"        clear_buffer (:obj:`bool`): Clears the no longer referenced "
"variables during backpropagation to save memory.\n"
"        accum_grad (:obj:`int`): Number of accumulation of gradients. "
"Update method of the `solver` is called after the `accum_grad` number of "
"the forward and backward is called.\n"
"        weight_decay (:obj:`float`): Decay constant. Default is `None`, "
"not applying the weight decay.\n"
"        comm (:obj:`nnabla.communicators.Communicator`): Communicator "
"when to do distributed training.\n"
"        grads (:obj:`list` of :obj:`nnabla._nd_array.NdArray`): The list "
"of gradients to be exchanged when to do distributed training.\n"
"\n"
"    Example:\n"
"\n"
"        .. code-block:: python\n"
"            solver = <Solver>\n"
"            loss = <Loss Variable of Network>\n"
"            data_feeder = <DataFeeder>\n"
"\n"
"            updater = DynamicLossScalingUpdater(solver, loss, "
"data_feeder)\n"
"\n"
"            # Training iteration\n"
"            for itr in range(max_iter):\n"
"                # Call solver.zero_grad, data_feeder, loss.forward, loss."
"backward\n"
"                # and solver.update with the dynamic loss scaling.\n"
"                updater.update()\n"
"\n"
"    Reference:\n"
"\n"
"        https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/"
"index.html#scalefactor\n"
"\n"
"    '''\n"
"\n"
"    def __init__(self, solver, loss, data_feeder=lambda x: x,\n"
"                  scale=8.0, scaling_factor=2.0, N=2000, "
"clear_buffer=True,\n"
"                  accum_grad=1, weight_decay=None,\n"
"                  comm=None,\n"
"                  grads=[]):\n"
"        self.solver = solver\n"
"        self.loss = loss\n"
"        self.data_feeder = data_feeder\n"
"        self.scale = scale\n"
"        self.scaling_factor = scaling_factor\n"
"        self.N = N\n"
"        self.clear_buffer = clear_buffer\n"
"        self.accum_grad = accum_grad\n"
"        self.weight_decay = weight_decay\n"
"        self.comm = comm\n"
"        self.grads = grads\n"
"        self._counter = 0\n"
"        self._recursive_count = 0\n"
"        self._max_recursive_count = 100\n"
"\n"
"    def update(self):\n"
"        \"\"\"Monolithic update method.\n"
"\n"
"        This method calls the following methods with the dynamic loss "
"scaling.\n"
"\n"
"        1. solver.zerograd\n"
"        2. feed data\n"
"        3. loss.forward\n"
"        4. loss.backward\n"
"        5. comm.all_reduce (if it is specified)\n"
"        6. solver.update\n"
"\n"
"        \"\"\"\n"
"\n"
"        # Initialize gradients.\n"
"        self.solver.zero_grad()\n"
"\n"
"        # Forward and backward\n"
"        for _ in range(self.accum_grad):\n"
"            # feed data\n"
"            self.data_feeder()\n"
"\n"
"            # forward\n"
"            self.loss.forward(clear_no_need_grad=self.clear_buffer)\n"
"\n"
"            # backward with scale\n"
"            self.loss.backward(self.scale, clear_buffer=self."
"clear_buffer)\n"
"\n"
"        # AllReduce\n"
"        if self.comm and len(self.grads) != 0:\n"
"            self.comm.all_reduce(self.grads, division=False, "
"inplace=False)\n"
"\n"
"        # Check Inf/NaN in grads\n"
"        if self.solver.check_inf_or_nan_grad():\n"
"            self.scale /= self.scaling_factor\n"
"            self._counter = 0\n"
"\n"
"            # Recursively call udpate function until no inf nor nan.\n"
"            self._recursive_count += 1\n"
"            if self._recursive_count > self._max_recursive_count:\n"
"                self._recursive_count = 0\n"
"                return  # skip\n"
"            return self.update()\n"
"        self._recursive_count = 0\n"
"\n"
"        # Rescale grads\n"
"        self.solver.scale_grad(1. / self.scale)\n"
"\n"
"        # Do some gradient clipping, etc.\n"
"        if self.weight_decay is not None:\n"
"            self.solver.weight_decay(self.weight_decay)\n"
"\n"
"        # Update\n"
"        self.solver.update()\n"
"        if self._counter > self.N:\n"
"            self.scale *= self.scaling_factor\n"
"            self._counter = 0\n"
"        self._counter += 1"
msgstr ""

#: ../../python/tutorial/mixed_precision_training.rst:229
msgid "Then, call the update method in a training loop:"
msgstr "その後、学習・ループ内で更新メソッドを呼びます。"

#: ../../python/tutorial/mixed_precision_training.rst:231
msgid ""
"from nnabla.experimental.mixed_precision_training import "
"DynamicLossScalingUpdater\n"
"\n"
"solver = <Solver>\n"
"loss = <Loss Variable of Network>\n"
"data_feeder = <DataFeeder>\n"
"\n"
"updater = DynamicLossScalingUpdater(solver, loss, data_feeder)\n"
"\n"
"# Training iteration\n"
"for itr in range(max_iter):\n"
"    # Call solver.zero_grad, data_feeder, loss.forward, loss.backward\n"
"    # and solver.update with the dynamic loss scaling.\n"
"    updater.update()"
msgstr ""

#: ../../python/tutorial/mixed_precision_training.rst:248
msgid "Notice"
msgstr "注意"

#: ../../python/tutorial/mixed_precision_training.rst:250
msgid "In the mixed-precision training, the followings are premise:"
msgstr "Mixed Precision を用いた学習では、次のような動作になります。"

#: ../../python/tutorial/mixed_precision_training.rst:252
msgid ""
"Solver contains ``FP16`` weights and the ``FP32`` copy of weights. "
"Solvers in NNabla hold ``FP32`` weights and weight gradients and cast it "
"to ``FP16`` weights in forward pass and to ``FP16`` weight gradients in "
"backward pass if one sets ``type_config=\"half\"``."
msgstr ""
"Solver は ``FP16`` の重みと ``FP32`` の重みのコピーを保持します。 "
"``type_config=\"half\"`` が指定されると、 NNabla における Solver は "
"``FP32`` の重みと重みの勾配を保持し、それを順方向パスで ``FP16`` の重みへ"
"キャストし、逆方向パスで ``FP16`` の重みの勾配へキャストします。"

#: ../../python/tutorial/mixed_precision_training.rst:257
msgid ""
"Reductions should be left in ``FP32``, for examples, the statistics (mean "
"and variance) computed by the batch-normalization, Mean, Sum, SoftMax, "
"SoftMaxCrossEntropy, etc. (see `The Training with Mixed Precision User "
"Guide <https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/"
"index.html>`__). In NNabla, these functions are automatically fallbacked "
"to use ``FP32``."
msgstr ""
"batch-normalization、 Mean、 Sum、 SoftMax、 SoftMaxCrossEntropy などに"
"よって計算される統計量 ( 平均や分散 ) の演算には、 ``FP32`` を使用してくだ"
"さい ( `混合精度学習のユーザーガイド <https://docs.nvidia.com/"
"deeplearning/sdk/mixed-precision-training/index.html>`__ を参照してくださ"
"い) 。 NNabla では、 type_config=\"half\" を指定した場合もこれらの関数は "
"自動的に ``FP32`` を使うようにフォールバックされます。"
