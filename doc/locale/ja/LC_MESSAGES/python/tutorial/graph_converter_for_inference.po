# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2017, Sony Corporation
# This file is distributed under the same license as the Neural Network
# Libraries package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2020.
#
msgid ""
msgstr ""
"Project-Id-Version: Neural Network Libraries 1.7.0.dev1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-06-10 15:10+0900\n"
"PO-Revision-Date: 2020-06-11 15:52+0900\n"
"Last-Translator: \n"
"Language: ja_JP\n"
"Language-Team: \n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"
"X-Generator: Poedit 2.3.1\n"

#: ../../python/tutorial/graph_converter_for_inference.rst:3
msgid "Graph Converter for Inference"
msgstr "推論のためのグラフコンバーター"

#: ../../python/tutorial/graph_converter_for_inference.rst:5
msgid ""
"In this tutorial, we demonstrate several graph converters mainly used for inference. Graph converters are basically used for a trained graph, "
"neural network, so once you train a neural network, you can use graph converters."
msgstr ""
"このチュートリアルでは、主に推論で使われているいくつかのグラフコンバーターを説明します。グラフコンバーターは基本的に学習済みのグラフ、ニューラ"
"ルネットワークに対して利用します。以下のグラフコンバーターについて、ユースケースに応じて使い方をステップバイステップで紹介します。"

#: ../../python/tutorial/graph_converter_for_inference.rst:10
msgid "We show how to use the following graph converters step-by-step according to usecases."
msgstr "次のようなグラフコンバーターの使い方を、ユースケースに応じて段階的に示します。"

#: ../../python/tutorial/graph_converter_for_inference.rst:13 ../../python/tutorial/graph_converter_for_inference.rst:57
msgid "BatchNormalizationLinearConverter"
msgstr "BatchNormalizationLinearConverter"

#: ../../python/tutorial/graph_converter_for_inference.rst:14 ../../python/tutorial/graph_converter_for_inference.rst:144
msgid "BatchNormalizationFoldedConverter"
msgstr "BatchNormalizationFoldedConverter"

#: ../../python/tutorial/graph_converter_for_inference.rst:15 ../../python/tutorial/graph_converter_for_inference.rst:212
msgid "FixedPointWeightConverter"
msgstr "FixedPointWeightConverter"

#: ../../python/tutorial/graph_converter_for_inference.rst:16 ../../python/tutorial/graph_converter_for_inference.rst:258
msgid "FixedPointActivationConverter"
msgstr "FixedPointActivationConverter"

#: ../../python/tutorial/graph_converter_for_inference.rst:18
msgid "**Note** before starting the following instruction, import python modules needed."
msgstr "**注** 先に実行に必要な Python モジュールをインポートしてください。"

#: ../../python/tutorial/graph_converter_for_inference.rst:21
msgid ""
"# Import\n"
"import numpy as np\n"
"import nnabla as nn\n"
"import nnabla.functions as F\n"
"import nnabla.parametric_functions as PF\n"
"\n"
"import nnabla.experimental.viewers as V\n"
"import nnabla.experimental.graph_converters as GC"
msgstr ""

#: ../../python/tutorial/graph_converter_for_inference.rst:32
msgid "Also, define LeNet as the motif."
msgstr "また、モチーフとして LeNet を定義します。"

#: ../../python/tutorial/graph_converter_for_inference.rst:34
msgid ""
"# LeNet\n"
"def LeNet(image, test=False):\n"
"    h = PF.convolution(image, 16, (5, 5), (1, 1), with_bias=False, name='conv1')\n"
"    h = PF.batch_normalization(h, batch_stat=not test, name='conv1-bn')\n"
"    h = F.max_pooling(h, (2, 2))\n"
"    h = F.relu(h)\n"
"\n"
"    h = PF.convolution(h, 16, (5, 5), (1, 1), with_bias=True, name='conv2')\n"
"    h = PF.batch_normalization(h, batch_stat=not test, name='conv2-bn')\n"
"    h = F.max_pooling(h, (2, 2))\n"
"    h = F.relu(h)\n"
"\n"
"    h = PF.affine(h, 10, with_bias=False, name='fc1')\n"
"    h = PF.batch_normalization(h, batch_stat=not test, name='fc1-bn')\n"
"    h = F.relu(h)\n"
"\n"
"    pred = PF.affine(h, 10, with_bias=True, name='fc2')\n"
"    return pred"
msgstr ""

#: ../../python/tutorial/graph_converter_for_inference.rst:59
msgid ""
"Typical networks contain the batch normalization layers. It serves as normalization in a network and uses the batch stats (the batch mean and "
"variance) to normalize inputs as"
msgstr ""
"典型的なネットワークには batch normalization 層があります。その層はネットワークにおいて正規化の役割を持ち、学習ではバッチ統計量 ( バッチの平均"
"と分散 ) を使って以下のように入力を正規化します。"

#: ../../python/tutorial/graph_converter_for_inference.rst:63
msgid "z = \\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta,"
msgstr "z = \\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta,"

#: ../../python/tutorial/graph_converter_for_inference.rst:68
msgid ""
"in training. :math:`\\mu` and :math:`\\sigma^2` are the batch mean and variance, and :math:`\\gamma` and :math:`\\beta` are the scale and bias "
"parameter to be learnt."
msgstr ""
":math:`\\mu` と :math:`\\sigma^2` はバッチの平均と分散で、 :math:`\\gamma` と :math:`\\beta` は学習対象の縮尺とバイアスのパラメータです。"

#: ../../python/tutorial/graph_converter_for_inference.rst:72
msgid ""
"At the same time, it computes the running stats (the exponential moving average :math:`\\mu_r` and variance :math:`\\sigma_r^2` of inputs to "
"the batch normalization layer), which are used later for inference."
msgstr ""
"同時に、移動統計量 ( batch normalization 層への入力の指数移動平均 :math:`\\mu_r` と分散 :math:`\\sigma_r^2` ) を計算し、それらは後で推論のため"
"に使われます。"

#: ../../python/tutorial/graph_converter_for_inference.rst:76
msgid "If nothing changes, in inference time, the batch normalization is performed as in the above equation using the running stats."
msgstr "特に変更がない場合、推論時の batch normalization では、移動統計量を使って上記の式と同様の正規化が行われます。"

#: ../../python/tutorial/graph_converter_for_inference.rst:79
msgid "z = \\gamma \\frac{x - \\mu_r}{\\sqrt{\\sigma_r^2 + \\epsilon}} + \\beta."
msgstr "z = \\gamma \\frac{x - \\mu_r}{\\sqrt{\\sigma_r^2 + \\epsilon}} + \\beta."

#: ../../python/tutorial/graph_converter_for_inference.rst:84
msgid ""
"This is the explicit normalization, so as you can see, there are many redundant computations (subtraction, devision, pow2, sqrt, "
"multiplication, addition) in inference, which should be avoided in inference graph. We can do it by ourselves, but it is apparently "
"troublesome."
msgstr ""
"この正規化では、推論中に冗長な計算 ( 引き算、割り算、 2 乗、平方根、掛け算、足し算 ) が多く見られ、推論グラフではこれを回避するべきです。自分た"
"ちで回避することもできますが、明らかに煩雑になります。"

#: ../../python/tutorial/graph_converter_for_inference.rst:90
msgid "BatchNormalizationLinearConverter automatically converts this equation of the batch normalization to the simple linear form as"
msgstr "BatchNormalizationLinearConverter は、自動的にこの式を以下のようなシンプルな線形演算に変換します。"

#: ../../python/tutorial/graph_converter_for_inference.rst:93
msgid ""
"z = c_0 x + c_1, \\\\ c_0 = \\frac{\\gamma}{\\sqrt{\\sigma_r^2 + \\epsilon}}, \\\\ c_1 = \\beta - \\frac{\\gamma \\mu_r}{\\sqrt{\\sigma_r^2 + "
"\\epsilon}}."
msgstr ""
"z = c_0 x + c_1, \\\\ c_0 = \\frac{\\gamma}{\\sqrt{\\sigma_r^2 + \\epsilon}}, \\\\ c_1 = \\beta - \\frac{\\gamma \\mu_r}{\\sqrt{\\sigma_r^2 + "
"\\epsilon}}."

#: ../../python/tutorial/graph_converter_for_inference.rst:100
msgid "After the conversion, we just have one multiplication and one addition since :math:`c_0` and :math:`c_1` can be precomputed in inference."
msgstr "変換後、 :math:`c_0` と :math:`c_1` は推論で事前に計算済みであり得るため、掛け算を 1 回と足し算を 1 回だけ行います。"

#: ../../python/tutorial/graph_converter_for_inference.rst:103
msgid ""
"Specifically, suppose that :math:`x` is the output of the 2D-Convolution, so :math:`x` is 3D-Tensor (e.g., :math:`N \\times H \\times W`). In "
"the batch normalization, the number of :math:`c`\\ s is the map size :math:`N`, respectively for :math:`c_0` and :math:`c_1`. Thus, the "
"multiplication (:math:`c_0 \\times x`) is :math:`N \\times H \\times W` and the addition :math:`c_0 \\times x + c_1` is same :math:`N \\times "
"H \\times W`. We can see much reduction compared to the native implementation."
msgstr ""
"つまり、 :math:`x` が 2D-Convolution の出力と仮定すると、 :math:`x` は 3D テンソル ( 例 :math:`N \\times H \\times W` ) となります。 Batch "
"normalization では、 :math:`c`\\ s の個数はそれぞれ :math:`c_0` と :math:`c_1` に対するマップサイズ :math:`N` です。従って、掛け算 ( :math:"
"`c_0 \\times x` ) は :math:`N \\times H \\times W` であり、足し算 :math:`c_0 \\times x + c_1` は同じく :math:`N \\times H \\times W` です。ネイ"
"ティブの実装と比べて多くの削減ができることがわかります。"

#: ../../python/tutorial/graph_converter_for_inference.rst:113 ../../python/tutorial/graph_converter_for_inference.rst:181
#: ../../python/tutorial/graph_converter_for_inference.rst:227 ../../python/tutorial/graph_converter_for_inference.rst:270
msgid "Example"
msgstr "例"

#: ../../python/tutorial/graph_converter_for_inference.rst:115 ../../python/tutorial/graph_converter_for_inference.rst:183
#: ../../python/tutorial/graph_converter_for_inference.rst:229 ../../python/tutorial/graph_converter_for_inference.rst:272
msgid "First, create LeNet."
msgstr "まず、 LeNet を作成します。"

#: ../../python/tutorial/graph_converter_for_inference.rst:117 ../../python/tutorial/graph_converter_for_inference.rst:185
#: ../../python/tutorial/graph_converter_for_inference.rst:231 ../../python/tutorial/graph_converter_for_inference.rst:274
msgid ""
"x = nn.Variable.from_numpy_array(np.random.rand(4, 3, 28, 28))\n"
"y = LeNet(x, test=True)"
msgstr ""

#: ../../python/tutorial/graph_converter_for_inference.rst:122 ../../python/tutorial/graph_converter_for_inference.rst:190
#: ../../python/tutorial/graph_converter_for_inference.rst:236 ../../python/tutorial/graph_converter_for_inference.rst:279
msgid "Now look at LeNet visually."
msgstr "次に、 LeNet を視覚化します。"

#: ../../python/tutorial/graph_converter_for_inference.rst:124 ../../python/tutorial/graph_converter_for_inference.rst:138
#: ../../python/tutorial/graph_converter_for_inference.rst:192 ../../python/tutorial/graph_converter_for_inference.rst:206
#: ../../python/tutorial/graph_converter_for_inference.rst:238 ../../python/tutorial/graph_converter_for_inference.rst:252
#: ../../python/tutorial/graph_converter_for_inference.rst:281 ../../python/tutorial/graph_converter_for_inference.rst:295
#: ../../python/tutorial/graph_converter_for_inference.rst:317
msgid ""
"viewer = V.SimpleGraph()\n"
"viewer.view(y)"
msgstr ""

#: ../../python/tutorial/graph_converter_for_inference.rst:129 ../../python/tutorial/graph_converter_for_inference.rst:197
#: ../../python/tutorial/graph_converter_for_inference.rst:243 ../../python/tutorial/graph_converter_for_inference.rst:286
msgid "Convert it to the one with the batch normalization linearly folded."
msgstr "LeNet をバッチ正規化をシンプルな線形演算にまとめたグラフに変換します。"

#: ../../python/tutorial/graph_converter_for_inference.rst:131
msgid ""
"converter = GC.BatchNormalizationLinearConverter(name=\"bn-linear-lenet\")\n"
"y = converter.convert(y, [x])"
msgstr ""

#: ../../python/tutorial/graph_converter_for_inference.rst:136 ../../python/tutorial/graph_converter_for_inference.rst:204
#: ../../python/tutorial/graph_converter_for_inference.rst:250 ../../python/tutorial/graph_converter_for_inference.rst:293
msgid "Also, show the converted graph."
msgstr "さらに、変換したグラフを表示します。"

#: ../../python/tutorial/graph_converter_for_inference.rst:146
msgid ""
"As you can see in the previous converter, BatchNormalizationLinearConverter is the linear folding of the batch normalization layer in "
"inference. However, if the preceding layer of the batch normalization is the convolution, affine or another layer performing inner-product, "
"that the linear folding is further folded into the weights of the preceding layers."
msgstr ""
"前述の BatchNormalizationLinearConverter を利用するとは、推論において batch normalization 層をシンプルな線形演算に変換することができます。しか"
"し、batch normalization の直前の層が convolution、 affine、または内積を行う他の層である場合、さらに それらの演算とまとめた線形演算に変換するこ"
"とができます。"

#: ../../python/tutorial/graph_converter_for_inference.rst:153
msgid "Suppose the sequence of a convolution and a batch normalization in inference, it can be written as,"
msgstr "推論において、 convolution と batch normalization のシーケンスを仮定すると、次のように記述することができます。"

#: ../../python/tutorial/graph_converter_for_inference.rst:156
msgid "z = c_0 \\times (w \\ast x + b) + c_1,"
msgstr "z = c_0 \\times (w \\ast x + b) + c_1,"

#: ../../python/tutorial/graph_converter_for_inference.rst:161
msgid ""
"where :math:`\\ast` is the convolutional operator, :math:`w` is the convolutional weights, and :math:`b` is the bias of the convolution layer. "
"Since :math:`\\ast` has linearity, we can further fold :math:`c_0` into the weights :math:`w` and bias :math:`b`, such that we have the "
"simpler form."
msgstr ""
"ここで、 :math:`\\ast` は畳み込みの演算子、 :math:`w` は畳み込みの重み、 :math:`b` は convolution 層のバイアスとします。 :math:`\\ast` は線形の"
"ため、よりシンプルな形になるように、重み :math:`w` とバイアス :math:`b` に :math:`c_0` をさらに畳み込むことができます。"

#: ../../python/tutorial/graph_converter_for_inference.rst:167
msgid "z = w' \\ast x + b', \\\\ w' = c_0 w, \\\\ b' = c_0 b + c_1."
msgstr "z = w' \\ast x + b', \\\\ w' = c_0 w, \\\\ b' = c_0 b + c_1."

#: ../../python/tutorial/graph_converter_for_inference.rst:174
msgid ""
"BatchNormalizationFoldedConverter automatically finds a sequence of the convolution and the batch normalization in a given graph, then folds "
"all parameters related to the batch normalization into the preceding convolution layer. Now, we do not need the multiplication and addition "
"seen in the previous case, BatchNormalizationLinearConverter."
msgstr ""
"BatchNormalizationFoldedConverter は、グラフ中の convolution と batch normalization のシーケンスを自動的に見つけ、 batch normalization に関連し"
"たすべてのパラメータを直前の convolution 層でまとめて表現します。ここでは、前述の BatchNormalizationLinearConverter で示された掛け算や足し算は"
"必要ありません。"

#: ../../python/tutorial/graph_converter_for_inference.rst:199
msgid ""
"converter = GC.BatchNormalizationFoldedConverter(name=\"bn-folded-lenet\")\n"
"y = converter.convert(y, [x])"
msgstr ""

#: ../../python/tutorial/graph_converter_for_inference.rst:214
msgid ""
"Once training finishes, where to deploy? Your destination of deployment of a trained model might be on Cloud or an embedded device. In either "
"case, the typical data type, FloatingPoint32 (FP32) might be redundant for inference, so you may want to use SIMD operation with e.g., 4-bit "
"or 8-bit of your target device. Training is usually performed using FP32, while interfence might be performed FixedPoint. Hence, you have to "
"change corresponding layers, e.g., the convolution and affine."
msgstr ""
"学習が終わると、どこにデプロイされるのでしょうか？学習済みモデルのデプロイ先はクラウドまたは組み込みデバイスかもしれません。どちらの場合も、 "
"FloatingPoint32 (FP32) のような典型的なデータ型は推定においては冗長である可能性があるため、対象デバイスの 4 ビットまたは 8 ビットなどの SIMD 演"
"算を使いたい場合があります。学習は一般的に FP32 を使って行われますが、推定は固定小数点を使う場合があります。したがって、 convolution や affine "
"などの対応する層を変更する必要があります。"

#: ../../python/tutorial/graph_converter_for_inference.rst:222
msgid ""
"FixedPointWeightConverter automatically converts the affine, convolution, and deconvolution of a given graph to that of fixed point version."
msgstr "FixedPointWeightConverter は、所定のグラフの affine、 convolution、または deconvolution を自動的に固定小数点のバージョンに変換します。"

#: ../../python/tutorial/graph_converter_for_inference.rst:245
msgid ""
"converter = GC.FixedPointWeightConverter(name=\"fixed-point-weight-lenet\")\n"
"y = converter.convert(y, [x])"
msgstr ""

#: ../../python/tutorial/graph_converter_for_inference.rst:260
msgid ""
"FixedPointWeightConverter converts layers of weights, but FixedPointActivationConverter automatically converts activation layers, e.g., ReLU. "
"The typial neural network architecture contains the sequence of the block ``ReLU -> Convolution -> BatchNormalization``; therefore, when you "
"convert both ``ReLU`` and ``Convolution`` to the fixed-point ones with proper hyper-paremters (step-size and bitwidth), you can utilize your "
"SIMD operation of your target device because both of the weights and inputs of the convolution are fixed-point."
msgstr ""
"FixedPointWeightConverter は重みを変換しますが、 FixedPointActivationConverter は ReLU などの活性化層を自動的に変換します。典型的なニューラル"
"ネットワークの構成は、 ``ReLU -> Convolution -> BatchNormalization`` のシーケンスを含んでいます。従って、 ``ReLU`` と ``Convolution`` の両方を"
"適切なハイパーパラメータ ( ステップサイズとビット幅 ) を持つ固定小数点に変換すれば、 Convolution の重みも入力も固定小数点となるので、対象デバイ"
"スの SIMD 演算を利用できます。"

#: ../../python/tutorial/graph_converter_for_inference.rst:288
msgid ""
"converter = GC.FixedPointActivationConverter(name=\"fixed-point-activation-lenet\")\n"
"y = converter.convert(y, [x])"
msgstr ""

#: ../../python/tutorial/graph_converter_for_inference.rst:300
msgid ""
"Tipically, FixedPointWeightConverter and FixedPointActivationConverter are used togather. For such purposes, you can use ``GC."
"SequentialConverter``."
msgstr ""
"通常、 FixedPointWeightConverter と FixedPointActivationConverter は併せて使います。そのようなケースでは、 ``GC.SequentialConverter`` を使うこ"
"とができます。"

#: ../../python/tutorial/graph_converter_for_inference.rst:304
msgid ""
"converter_w = GC.FixedPointWeightConverter(name=\"fixed-point-lenet\")\n"
"converter_a = GC.FixedPointActivationConverter(name=\"fixed-point-lenet\")\n"
"converter = GC.SequentialConverter([converter_w, converter_a])\n"
"y = converter.convert(y, [x])"
msgstr ""

#: ../../python/tutorial/graph_converter_for_inference.rst:311
msgid ""
"Needless to say, ``GC.SequentialConverter`` is not limited to using this case. One you creat your own ``Conveterter``\\ s, then you can add "
"these converters to ``GC.SequentialConverter`` if these are used togather."
msgstr ""
"言うまでもなく、 ``GC.SequentialConverter`` はこのような使用に限定されません。独自のコンバーターを作成し、 ``GC.SequentialConverter`` に追加す"
"ることで複数の ``Conveterter`` を同時に利用することができます。"

#: ../../python/tutorial/graph_converter_for_inference.rst:315
msgid "Look at the converted graph visually."
msgstr "変換されたグラフを視覚化します。"
