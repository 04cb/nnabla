# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2017, Sony Corporation
# This file is distributed under the same license as the Neural Network
# Libraries package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2020.
#
msgid ""
msgstr ""
"Project-Id-Version: Neural Network Libraries 1.7.0.dev1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-05-27 10:13+0900\n"
"PO-Revision-Date: 2020-05-14 11:13+0900\n"
"Last-Translator: \n"
"Language: ja_JP\n"
"Language-Team: \n"
"Plural-Forms: nplurals=1; plural=0\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../python/tutorial/dynamic_and_static_nn.rst:3
msgid "Static vs Dynamic Neural Networks in NNabla"
msgstr "NNabla における静的 vs 動的ニューラルネットワーク"

#: ../../python/tutorial/dynamic_and_static_nn.rst:5
msgid ""
"NNabla allows you to define static and dynamic neural networks. Static "
"neural networks have a fixed layer architecture, i.e., a static "
"computation graph. In contrast, dynamic neural networks use a dynamic "
"computation graph, e.g., randomly dropping layers for each minibatch."
msgstr ""
"NNabla "
"では、静的ニューラルと動的ニューラルネットワークを定義することができます。静的ニューラルネットワークは固定した層の構造、つまり、静的計算グラフを持ちます。対照的に、動的ニューラルネットワークは、例えばそれぞれのミニバッチに対してランダムに層をドロップするような動的計算グラフを使います。"

#: ../../python/tutorial/dynamic_and_static_nn.rst:10
msgid "This tutorial compares both computation graphs."
msgstr "このチュートリアルでは、 2 つの計算グラフを比較します。"

#: ../../python/tutorial/dynamic_and_static_nn.rst:12
msgid ""
"%matplotlib inline\n"
"import nnabla as nn\n"
"import nnabla.functions as F\n"
"import nnabla.parametric_functions as PF\n"
"import nnabla.solvers as S\n"
"\n"
"import numpy as np\n"
"np.random.seed(0)\n"
"\n"
"GPU = 0  # ID of GPU that we will use"
msgstr ""

#: ../../python/tutorial/dynamic_and_static_nn.rst:28
msgid "2017-06-26 23:10:05,832 [nnabla][INFO]: Initializing CPU extension..."
msgstr ""

#: ../../python/tutorial/dynamic_and_static_nn.rst:32
msgid "Dataset loading"
msgstr "データセットの読み込み"

#: ../../python/tutorial/dynamic_and_static_nn.rst:34
msgid "We will first setup the digits dataset from scikit-learn:"
msgstr "まず、 scikit-learn から数字のデータセットをセットアップしましょう。"

#: ../../python/tutorial/dynamic_and_static_nn.rst:36
msgid ""
"from tiny_digits import *\n"
"\n"
"digits = load_digits()\n"
"data = data_iterator_tiny_digits(digits, batch_size=16, shuffle=True)"
msgstr ""

#: ../../python/tutorial/dynamic_and_static_nn.rst:46
msgid ""
"2017-06-26 23:10:06,042 [nnabla][INFO]: DataSource with shuffle(True)\n"
"2017-06-26 23:10:06,043 [nnabla][INFO]: Using DataSourceWithMemoryCache\n"
"2017-06-26 23:10:06,044 [nnabla][INFO]: DataSource with shuffle(True)\n"
"2017-06-26 23:10:06,044 [nnabla][INFO]: On-memory\n"
"2017-06-26 23:10:06,045 [nnabla][INFO]: Using DataIterator"
msgstr ""

#: ../../python/tutorial/dynamic_and_static_nn.rst:53
msgid ""
"Each sample in this dataset is a grayscale image of size 8x8 and belongs "
"to one of the ten classes ``0``, ``1``, ..., ``9``."
msgstr ""
"このデータセットにあるサンプルは、それぞれ 8 x 8 サイズのグレースケール画像で、 ``0``、 ``1``、 … 、 ``9`` の 10 "
"クラスの 1 つに属しています。"

#: ../../python/tutorial/dynamic_and_static_nn.rst:56
msgid ""
"img, label = data.next()\n"
"print(img.shape, label.shape)"
msgstr ""

#: ../../python/tutorial/dynamic_and_static_nn.rst:64
msgid "(16, 1, 8, 8) (16, 1)"
msgstr ""

#: ../../python/tutorial/dynamic_and_static_nn.rst:68
msgid "Network definition"
msgstr "ネットワークの定義"

#: ../../python/tutorial/dynamic_and_static_nn.rst:70
msgid "As an example, we define a (unnecessarily) deep CNN:"
msgstr "例として、 ( 不必要に深層である ) 深層 CNN を定義します。"

#: ../../python/tutorial/dynamic_and_static_nn.rst:72
msgid ""
"def cnn(x):\n"
"    \"\"\"Unnecessarily Deep CNN.\n"
"\n"
"    Args:\n"
"        x : Variable, shape (B, 1, 8, 8)\n"
"\n"
"    Returns:\n"
"        y : Variable, shape (B, 10)\n"
"    \"\"\"\n"
"    with nn.parameter_scope(\"cnn\"):  # Parameter scope can be nested\n"
"        with nn.parameter_scope(\"conv1\"):\n"
"            h = F.tanh(PF.batch_normalization(\n"
"                PF.convolution(x, 64, (3, 3), pad=(1, 1))))\n"
"        for i in range(10):  # unnecessarily deep\n"
"            with nn.parameter_scope(\"conv{}\".format(i + 2)):\n"
"                h = F.tanh(PF.batch_normalization(\n"
"                    PF.convolution(h, 128, (3, 3), pad=(1, 1))))\n"
"        with nn.parameter_scope(\"conv_last\"):\n"
"            h = F.tanh(PF.batch_normalization(\n"
"                PF.convolution(h, 512, (3, 3), pad=(1, 1))))\n"
"            h = F.average_pooling(h, (2, 2))\n"
"        with nn.parameter_scope(\"fc\"):\n"
"            h = F.tanh(PF.affine(h, 1024))\n"
"        with nn.parameter_scope(\"classifier\"):\n"
"            y = PF.affine(h, 10)\n"
"    return y"
msgstr ""

#: ../../python/tutorial/dynamic_and_static_nn.rst:102
msgid "Static computation graph"
msgstr "静的計算グラフ"

#: ../../python/tutorial/dynamic_and_static_nn.rst:104
msgid ""
"First, we will look at the case of a static computation graph where the "
"neural network does not change during training."
msgstr "まず、学習中にニューラルネットワークが変化しない、静的計算グラフの場合を見ましょう。"

#: ../../python/tutorial/dynamic_and_static_nn.rst:107
msgid ""
"from nnabla.ext_utils import get_extension_context\n"
"\n"
"# setup cuda extension\n"
"ctx_cuda = get_extension_context('cudnn', device_id=GPU)  # replace "
"'cudnn' by 'cpu' if you want to run the example on the CPU\n"
"nn.set_default_context(ctx_cuda)\n"
"\n"
"# create variables for network input and label\n"
"x = nn.Variable(img.shape)\n"
"t = nn.Variable(label.shape)\n"
"\n"
"# create network\n"
"static_y = cnn(x)\n"
"static_y.persistent = True\n"
"\n"
"# define loss function for training\n"
"static_l = F.mean(F.softmax_cross_entropy(static_y, t))"
msgstr ""

#: ../../python/tutorial/dynamic_and_static_nn.rst:129
msgid ""
"2017-06-26 23:10:06,350 [nnabla][INFO]: Initializing CUDA extension...\n"
"2017-06-26 23:10:06,571 [nnabla][INFO]: Initializing cuDNN extension..."
msgstr ""

#: ../../python/tutorial/dynamic_and_static_nn.rst:133
msgid "Setup solver for training"
msgstr "学習のための solver のセットアップ"

#: ../../python/tutorial/dynamic_and_static_nn.rst:135
msgid ""
"solver = S.Adam(alpha=1e-3)\n"
"solver.set_parameters(nn.get_parameters())"
msgstr ""

#: ../../python/tutorial/dynamic_and_static_nn.rst:140
msgid "Create data iterator"
msgstr "データ反復子の作成"

#: ../../python/tutorial/dynamic_and_static_nn.rst:142
msgid ""
"loss = []\n"
"def epoch_end_callback(epoch):\n"
"    global loss\n"
"    print(\"[{} {} {}]\".format(epoch, np.mean(loss), itr))\n"
"    loss = []\n"
"\n"
"data = data_iterator_tiny_digits(digits, batch_size=16, shuffle=True)\n"
"data.register_epoch_end_callback(epoch_end_callback)"
msgstr ""

#: ../../python/tutorial/dynamic_and_static_nn.rst:156
msgid ""
"2017-06-26 23:10:07,221 [nnabla][INFO]: DataSource with shuffle(True)\n"
"2017-06-26 23:10:07,224 [nnabla][INFO]: Using DataSourceWithMemoryCache\n"
"2017-06-26 23:10:07,226 [nnabla][INFO]: DataSource with shuffle(True)\n"
"2017-06-26 23:10:07,228 [nnabla][INFO]: On-memory\n"
"2017-06-26 23:10:07,230 [nnabla][INFO]: Using DataIterator"
msgstr ""

#: ../../python/tutorial/dynamic_and_static_nn.rst:163
msgid "Perform training iterations and output training loss:"
msgstr "学習の反復の実行と学習ロスの出力。"

#: ../../python/tutorial/dynamic_and_static_nn.rst:165
#, python-format
msgid ""
"%%time\n"
"for epoch in range(30):\n"
"    itr = 0\n"
"    while data.epoch == epoch:\n"
"        x.d, t.d = data.next()\n"
"        static_l.forward(clear_no_need_grad=True)\n"
"        solver.zero_grad()\n"
"        static_l.backward(clear_buffer=True)\n"
"        solver.update()\n"
"        loss.append(static_l.d.copy())\n"
"        itr += 1\n"
"print()"
msgstr ""

#: ../../python/tutorial/dynamic_and_static_nn.rst:183
msgid ""
"[ 0 0.909297 112 ] [ 1 0.183863 111 ] [ 2 0.0723054 111 ] [ 3 0.0653021 "
"112 ] [ 4 0.0628503 111 ] [ 5 0.0731626 111 ] [ 6 0.0319093 112 ] [ 7 "
"0.0610926 111 ] [ 8 0.0817437 111 ] [ 9 0.0717577 112 ] [ 10 0.0241882 "
"111 ] [ 11 0.0119452 111 ] [ 12 0.00664761 112 ] [ 13 0.00377711 111 ] [ "
"14 0.000605656 111 ] [ 15 0.000236613 111 ] [ 16 0.000174549 112 ] [ 17 "
"0.000142428 111 ] [ 18 0.000126015 111 ] [ 19 0.000111144 112 ] [ 20 "
"0.000100751 111 ] [ 21 9.03808e-05 111 ] [ 22 8.35904e-05 112 ] [ 23 "
"7.73492e-05 111 ] [ 24 6.91389e-05 111 ] [ 25 6.74929e-05 112 ] [ 26 "
"6.08386e-05 111 ] [ 27 5.62182e-05 111 ] [ 28 5.33428e-05 112 ] [ 29 "
"4.94594e-05 111 ]\n"
"CPU times: user 14.3 s, sys: 6.78 s, total: 21.1 s\n"
"Wall time: 21.1 s"
msgstr ""

#: ../../python/tutorial/dynamic_and_static_nn.rst:189
msgid "Dynamic computation graph"
msgstr "動的計算グラフ"

#: ../../python/tutorial/dynamic_and_static_nn.rst:191
msgid ""
"Now, we will use a dynamic computation graph, where the neural network is"
" setup each time we want to do a forward/backward pass through it. This "
"allows us to, e.g., randomly dropout layers or to have network "
"architectures that depend on input data. In this example, we will use for"
" simplicity the same neural network structure and only dynamically create"
" it. For example, adding a ``if np.random.rand() > dropout_probability:``"
" into ``cnn()`` allows to dropout layers."
msgstr ""
"次に、動的計算グラフを使います。ここでは、ニューラルネットワークを通して順方向 / "
"逆方向パスをしたい場合、毎回ニューラルネットワークがセットアップされます。これにより、例えば、ランダムに層をドロップアウトしたり、入力データに依存するネットワーク構造を持つことができます。ここでのサンプルでは、簡略化のため、同じニューラルネットワーク構造を使い、動的な生成のみを行います。例えば、"
" ``cnn()`` に ``if np.random.rand() > dropout_probability:`` "
"を加えると、層をドロップアウトできます。"

#: ../../python/tutorial/dynamic_and_static_nn.rst:200
msgid "First, we setup the solver and the data iterator for the training:"
msgstr "まず、学習のために solver とデータ反復子をセットアップします。"

#: ../../python/tutorial/dynamic_and_static_nn.rst:202
msgid ""
"nn.clear_parameters()\n"
"solver = S.Adam(alpha=1e-3)\n"
"solver.set_parameters(nn.get_parameters())\n"
"\n"
"loss = []\n"
"def epoch_end_callback(epoch):\n"
"    global loss\n"
"    print(\"[{} {} {}]\".format(epoch, np.mean(loss), itr))\n"
"    loss = []\n"
"data = data_iterator_tiny_digits(digits, batch_size=16, shuffle=True)\n"
"data.register_epoch_end_callback(epoch_end_callback)"
msgstr ""

#: ../../python/tutorial/dynamic_and_static_nn.rst:219
msgid ""
"2017-06-26 23:10:28,449 [nnabla][INFO]: DataSource with shuffle(True)\n"
"2017-06-26 23:10:28,450 [nnabla][INFO]: Using DataSourceWithMemoryCache\n"
"2017-06-26 23:10:28,450 [nnabla][INFO]: DataSource with shuffle(True)\n"
"2017-06-26 23:10:28,451 [nnabla][INFO]: On-memory\n"
"2017-06-26 23:10:28,451 [nnabla][INFO]: Using DataIterator"
msgstr ""

#: ../../python/tutorial/dynamic_and_static_nn.rst:226
#, python-format
msgid ""
"%%time\n"
"for epoch in range(30):\n"
"    itr = 0\n"
"    while data.epoch == epoch:\n"
"        x.d, t.d = data.next()\n"
"        with nn.auto_forward():\n"
"            dynamic_y = cnn(x)\n"
"            dynamic_l = F.mean(F.softmax_cross_entropy(dynamic_y, t))\n"
"        solver.set_parameters(nn.get_parameters(), reset=False, "
"retain_state=True) # this can be done dynamically\n"
"        solver.zero_grad()\n"
"        dynamic_l.backward(clear_buffer=True)\n"
"        solver.update()\n"
"        loss.append(dynamic_l.d.copy())\n"
"        itr += 1\n"
"print()"
msgstr ""

#: ../../python/tutorial/dynamic_and_static_nn.rst:247
msgid ""
"[ 0 1.04669 112 ] [ 1 0.151949 111 ] [ 2 0.093581 111 ] [ 3 0.129242 112 "
"] [ 4 0.0452591 111 ] [ 5 0.0343987 111 ] [ 6 0.0315372 112 ] [ 7 "
"0.0336886 111 ] [ 8 0.0194571 111 ] [ 9 0.00923094 112 ] [ 10 0.00536065 "
"111 ] [ 11 0.000669383 111 ] [ 12 0.000294232 112 ] [ 13 0.000245866 111 "
"] [ 14 0.000201116 111 ] [ 15 0.000164177 111 ] [ 16 0.00014832 112 ] [ "
"17 0.000131479 111 ] [ 18 0.000115171 111 ] [ 19 0.000101432 112 ] [ 20 "
"9.06228e-05 111 ] [ 21 8.7103e-05 111 ] [ 22 7.79601e-05 112 ] [ 23 "
"7.59678e-05 111 ] [ 24 6.64341e-05 111 ] [ 25 6.22717e-05 112 ] [ 26 "
"5.8643e-05 111 ] [ 27 5.35373e-05 111 ] [ 28 4.96717e-05 112 ] [ 29 "
"4.65124e-05 111 ]\n"
"CPU times: user 23.4 s, sys: 5.35 s, total: 28.7 s\n"
"Wall time: 28.7 s"
msgstr ""

#: ../../python/tutorial/dynamic_and_static_nn.rst:252
msgid ""
"Comparing the two processing times, we can observe that both schemes "
"(\"static\" and \"dynamic\") takes the same execution time, i.e., "
"although we created the computation graph dynamically, we did not lose "
"performance."
msgstr ""
"2 つの処理時間を比べると、 ( “ 静的” も “動的” も ) "
"どちらのスキームも同じ実行時間がかかることがわかります。つまり、計算グラフを動的に作成しても、パフォーマンスは下がらないということです。"

