# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2017, Sony Corporation
# This file is distributed under the same license as the Neural Network
# Libraries package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2020.
#
msgid ""
msgstr ""
"Project-Id-Version: Neural Network Libraries 1.7.0.dev1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-06-10 15:10+0900\n"
"PO-Revision-Date: 2020-06-11 15:46+0900\n"
"Last-Translator: \n"
"Language: ja_JP\n"
"Language-Team: \n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"
"X-Generator: Poedit 2.3.1\n"

#: ../../python/tutorial/by_examples.rst:3
msgid "NNabla by Examples"
msgstr "サンプルで学ぶ NNabla"

#: ../../python/tutorial/by_examples.rst:5
msgid "This tutorial demonstrates how you can write a script to train a neural network by using a simple hand digits classification task."
msgstr ""
"このチュートリアルでは、簡単な手書きの数字を分類するタスクを例に、ニューラルネットワークの学習を行うスクリプトの書き方を見ていきます。"

#: ../../python/tutorial/by_examples.rst:8
msgid ""
"Note: This tutorial notebook requires `scikit-learn <http://scikit-learn.org>`__ and `matplotlib <https://matplotlib.org/>`__ installed "
"in your Python environment."
msgstr ""
"注 : このチュートリアルでは、 `scikit-learn <http://scikit-learn.org>`__ と `matplotlib <https://matplotlib.org/>`__ がインストールされ"
"た Python 環境が必要です。"

#: ../../python/tutorial/by_examples.rst:13
msgid "First let us prepare some dependencies."
msgstr "まず、依存関係を準備しましょう。"

#: ../../python/tutorial/by_examples.rst:15
msgid ""
"import nnabla as nn\n"
"\n"
"import nnabla.functions as F\n"
"import nnabla.parametric_functions as PF\n"
"import nnabla.solvers as S\n"
"from nnabla.monitor import tile_images\n"
"\n"
"import numpy as np\n"
"import matplotlib.pyplot as plt\n"
"import tiny_digits\n"
"%matplotlib inline\n"
"\n"
"np.random.seed(0)\n"
"imshow_opt = dict(cmap='gray', interpolation='nearest')"
msgstr ""
"import nnabla as nn \n"
"\n"
"import nnabla.functions as F \n"
"import nnabla.parametric_functions as PF \n"
"import nnabla.solvers as S \n"
"from nnabla.monitor import tile_images \n"
"\n"
"import numpy as np \n"
"import matplotlib.pyplot as plt \n"
"import tiny_digits \n"
"%matplotlib inline \n"
"\n"
"np.random.seed(0) \n"
"imshow_opt = dict(cmap='gray', interpolation='nearest')"

#: ../../python/tutorial/by_examples.rst:35
msgid "2017-06-26 23:09:49,971 [nnabla][INFO]: Initializing CPU extension..."
msgstr "2017-06-26 23:09:49,971 [nnabla][INFO]: Initializing CPU extension..."

#: ../../python/tutorial/by_examples.rst:38
msgid ""
"The ``tiny_digits`` module is located under this folder. It provides some utilities for loading a handwritten-digit classification "
"dataset (MNIST) available in scikit-learn."
msgstr ""
"``tiny_digits`` モジュールはこのフォルダにあります。このモジュールは、scikit-learn で利用可能な手書き数字のデータセット ( MNIST ) をロー"
"ドするためのユーティリティーを提供します。"

#: ../../python/tutorial/by_examples.rst:43
msgid "Logistic Regression"
msgstr "ロジスティック回帰"

#: ../../python/tutorial/by_examples.rst:45
msgid "We will first start by defining a computation graph for logistic regression. (For details on logistic regression, see Appendix A.)"
msgstr ""
"まず、ロジスティック回帰のための計算グラフを定義するところから始めます。 ( ロジスティック回帰の詳細は付録 A を参照してください。 )"

#: ../../python/tutorial/by_examples.rst:48
msgid ""
"The training will be done by gradient descent, where gradients are calculated using the error backpropagation algorithm (backprop)."
msgstr "学習は勾配降下法を用いて行われます。勾配は得られた誤差をバックプロパゲーションしていく ( backprop ) ことで算出されます。"

#: ../../python/tutorial/by_examples.rst:52
msgid "Preparing a Toy Dataset"
msgstr "トイ・データセットの準備"

#: ../../python/tutorial/by_examples.rst:54
msgid "This section just prepares a dataset to be used for demonstration of NNabla usage."
msgstr "ここではこのサンプルで用いるデータセットの準備を行います。"

#: ../../python/tutorial/by_examples.rst:57
msgid ""
"digits = tiny_digits.load_digits(n_class=10)\n"
"tiny_digits.plot_stats(digits)"
msgstr ""
"digits = tiny_digits.load_digits(n_class=10) \n"
"tiny_digits.plot_stats(digits)"

#: ../../python/tutorial/by_examples.rst:65
msgid ""
"Num images: 1797\n"
"Image shape: (8, 8)\n"
"Labels: [0 1 2 3 4 5 6 7 8 9]"
msgstr ""
"Num images: 1797 \n"
"Image shape: (8, 8) \n"
"Labels: [0 1 2 3 4 5 6 7 8 9]"

#: ../../python/tutorial/by_examples.rst:74
msgid ""
"The next block creates a dataset loader which is a generator providing images and labels as minibatches. Note that this dataset is just "
"an example purpose and not a part of NNabla."
msgstr ""
"次のブロックでは、データセットから画像やラベルをミニバッチ分だけ返すデータローダーを作成します。このデータセットはこのサンプル用に利用し"
"ているものであり、 NNabla の一部ではないことに注意してください。"

#: ../../python/tutorial/by_examples.rst:78
msgid "data = tiny_digits.data_iterator_tiny_digits(digits, batch_size=64, shuffle=True)"
msgstr "data = tiny_digits.data_iterator_tiny_digits(digits, batch_size=64, shuffle=True)"

#: ../../python/tutorial/by_examples.rst:85
msgid ""
"2017-06-26 23:09:50,545 [nnabla][INFO]: DataSource with shuffle(True)\n"
"2017-06-26 23:09:50,546 [nnabla][INFO]: Using DataSourceWithMemoryCache\n"
"2017-06-26 23:09:50,546 [nnabla][INFO]: DataSource with shuffle(True)\n"
"2017-06-26 23:09:50,547 [nnabla][INFO]: On-memory\n"
"2017-06-26 23:09:50,547 [nnabla][INFO]: Using DataIterator"
msgstr ""
"2017-06-26 23:09:50,545 [nnabla][INFO]: DataSource with shuffle(True) \n"
"2017-06-26 23:09:50,546 [nnabla][INFO]: Using DataSourceWithMemoryCache \n"
"2017-06-26 23:09:50,546 [nnabla][INFO]: DataSource with shuffle(True) \n"
"2017-06-26 23:09:50,547 [nnabla][INFO]: On-memory \n"
"2017-06-26 23:09:50,547 [nnabla][INFO]: Using DataIterator"

#: ../../python/tutorial/by_examples.rst:92
msgid "A minibatch is as follows. ``img`` and ``label`` are in ``numpy.ndarray``."
msgstr "ミニバッチは次のようになります。``img`` と ``label`` は ``numpy.ndarray`` として得られます。"

#: ../../python/tutorial/by_examples.rst:95
msgid ""
"img, label = data.next()\n"
"plt.imshow(tile_images(img), **imshow_opt)\n"
"print(\"labels: {}\".format(label.reshape(8, 8)))\n"
"print(\"Label shape: {}\".format(label.shape))"
msgstr ""
"img, label = data.next() \n"
"plt.imshow(tile_images(img), **imshow_opt) \n"
"print(\"labels: {}\".format(label.reshape(8, 8))) \n"
"print(\"Label shape: {}\".format(label.shape))"

#: ../../python/tutorial/by_examples.rst:105
msgid ""
"labels: [[ 2.  8.  2.  6.  6.  7.  1.  9.]\n"
" [ 8.  5.  2.  8.  6.  6.  6.  6.]\n"
" [ 1.  0.  5.  8.  8.  7.  8.  4.]\n"
" [ 7.  5.  4.  9.  2.  9.  4.  7.]\n"
" [ 6.  8.  9.  4.  3.  1.  0.  1.]\n"
" [ 8.  6.  7.  7.  1.  0.  7.  6.]\n"
" [ 2.  1.  9.  6.  7.  9.  0.  0.]\n"
" [ 5.  1.  6.  3.  0.  2.  3.  4.]]\n"
"Label shape: (64, 1)"
msgstr ""
"labels: [[ 2.  8.  2.  6.  6.  7.  1.  9.] \n"
" [ 8.  5.  2.  8.  6.  6.  6.  6.] \n"
" [ 1.  0.  5.  8.  8.  7.  8.  4.] \n"
" [ 7.  5.  4.  9.  2.  9.  4.  7.] \n"
" [ 6.  8.  9.  4.  3.  1.  0.  1.] \n"
" [ 8.  6.  7.  7.  1.  0.  7.  6.] \n"
" [ 2.  1.  9.  6.  7.  9.  0.  0.] \n"
" [ 5.  1.  6.  3.  0.  2.  3.  4.]] \n"
"Label shape: (64, 1)"

#: ../../python/tutorial/by_examples.rst:121
msgid "Preparing the Computation Graph"
msgstr "計算グラフの準備"

#: ../../python/tutorial/by_examples.rst:123
msgid ""
"NNabla provides two different ways for backprop-based gradient descent optimization. One is with a static graph, and another is with a "
"dynamic graph. We are going to show a static version first."
msgstr ""
"NNabla では 誤差逆伝播法を用いる勾配降下法の最適化について 2 つの手法を提供しています。1 つは静的グラフで、もう 1 つは動的グラフによるも"
"のです。初めに、静的な方から見ていきましょう。"

#: ../../python/tutorial/by_examples.rst:127
msgid ""
"# Forward pass\n"
"x = nn.Variable(img.shape)  # Define an image variable\n"
"with nn.parameter_scope(\"affine1\"):\n"
"    y = PF.affine(x, 10)  # Output is 10 class"
msgstr ""
"# Forward pass\n"
"x = nn.Variable(img.shape)  # Define an image variable\n"
"with nn.parameter_scope(\"affine1\"):\n"
"    y = PF.affine(x, 10)  # Output is 10 class"

#: ../../python/tutorial/by_examples.rst:134
msgid ""
"This code block shows one of the most important features in graph building in NNabla, the **parameter scope**. The first line defines "
"an input variable ``x``. The second line creates a **parameter scope**. The third line then applies ``PF.affine`` - an affine transform "
"- to ``x``, and creates a variable ``y`` holding that result. Here, the ``PF`` (parametric\\_function) module provides functions that "
"contain learnable parameters, such as affine transforms (which contains weights), convolution (which contains kernels) and batch "
"normalization (which contains transformation factors and coefficients). We will call these functions as **parametric functions**. The "
"parameters are created and initialized randomly at function call, and registered by a name \"affine1\" using ``parameter_scope`` "
"context."
msgstr ""
"このコードブロックは NNabla のグラフ作成機能の中で最も重要な特徴の 1 つである **パラメータスコープの使い方** を示しています。1 行目で入"
"力変数 ``x`` を定義します。2 行目で **パラメータスコープ** を作成します。 そして、3 行目で ``x`` に対してaffine変換 ``PF.affine`` を適用"
"して、その結果を保持する変数 ``y`` を作ります。ここで、 ``PF`` ( parametric\\_function ) モジュールは ( 重みを含む ) affine変換、( カー"
"ネルを含む ) 畳み込み、( スケール因子など ) バッチ正規化のような、学習によって更新したいパラメータを含む関数を提供します。これらの関数"
"を **パラメトリック関数** といいます。 パラメータは関数呼び出しで作られ、ランダムに初期化され、 ``parameter_scope`` コンテキストを使っ"
"て \"affine1\" という名前で登録されます。"

#: ../../python/tutorial/by_examples.rst:147
msgid ""
"# Building a loss graph\n"
"t = nn.Variable(label.shape)  # Define an target variable\n"
"loss = F.mean(F.softmax_cross_entropy(y, t))  # Softmax Xentropy fits multi-class classification problems"
msgstr ""
"# Building a loss graph\n"
"t = nn.Variable(label.shape)  # Define an target variable\n"
"loss = F.mean(F.softmax_cross_entropy(y, t))  # Softmax Xentropy fits multi-class classification problems"

#: ../../python/tutorial/by_examples.rst:153
msgid ""
"The remaining lines shown above define a target variable and attach functions for loss at the end of the graph. Note that the static "
"graph build doesn't execute any computation, but the shapes of output variables are inferred. Therefore, we can inspect the shapes of "
"each variable at this time:"
msgstr ""
"上記の残りの行では、目的関数を定義して、グラフの終端にロス関数を追加しています。静的グラフを構築しても、定義した計算は実際には実行されま"
"せんが、出力変数の状態は推測されることに注意してください。したがって、この時にそれぞれの変数の状態を調べることができます。"

#: ../../python/tutorial/by_examples.rst:159
msgid ""
"print(\"Printing shapes of variables\")\n"
"print(x.shape)\n"
"print(y.shape)\n"
"print(t.shape)\n"
"print(loss.shape)  # empty tuple means scalar"
msgstr ""
"print(\"Printing shapes of variables\")\n"
"print(x.shape)\n"
"print(y.shape)\n"
"print(t.shape)\n"
"print(loss.shape)  # empty tuple means scalar"

#: ../../python/tutorial/by_examples.rst:170
msgid ""
"Printing shapes of variables\n"
"(64, 1, 8, 8)\n"
"(64, 10)\n"
"(64, 1)\n"
"()"
msgstr ""
"Printing shapes of variables \n"
"(64, 1, 8, 8) \n"
"(64, 10) \n"
"(64, 1) \n"
"()"

#: ../../python/tutorial/by_examples.rst:178
msgid "Executing a static graph"
msgstr "静的グラフの実行"

#: ../../python/tutorial/by_examples.rst:180
msgid ""
"You can execute the computation of the graph by calling the ``forward()`` method in a sink variable. Inputs can be set via ``.d`` "
"accessor. It will borrow CPU array references as ``numpy.ndarray``."
msgstr ""
"グラフの終端である変数にある ``forward()`` メソッドを呼び出すことによって、グラフで定義された計算を実行することができます。入力は ``."
"d`` アクセサによってセットすることができます。このアクセサは ``numpy.ndarray`` として CPU から配列を受け取ります。"

#: ../../python/tutorial/by_examples.rst:184
msgid ""
"# Set data\n"
"x.d = img\n"
"t.d = label\n"
"# Execute a forward pass\n"
"loss.forward()\n"
"# Showing results\n"
"print(\"Prediction score of 0-th image: {}\".format(y.d[0]))\n"
"print(\"Loss: {}\".format(loss.d))"
msgstr ""
"# Set data\n"
"x.d = img\n"
"t.d = label\n"
"# Execute a forward pass\n"
"loss.forward()\n"
"# Showing results\n"
"print(\"Prediction score of 0-th image: {}\".format(y.d[0]))\n"
"print(\"Loss: {}\".format(loss.d))"

#: ../../python/tutorial/by_examples.rst:198
msgid ""
"Prediction score of 0-th image: [  9.75851917   6.49118519  16.47323608  -1.36296904  -0.78583491\n"
"   4.08872032   7.84134388   2.42956853   3.31485462   3.61868763]\n"
"Loss: 10.6016616821"
msgstr ""
"Prediction score of 0-th image: [  9.75851917   6.49118519  16.47323608  -1.36296904  -0.78583491 \n"
"   4.08872032   7.84134388   2.42956853   3.31485462   3.61868763] \n"
"Loss: 10.6016616821"

#: ../../python/tutorial/by_examples.rst:203
msgid "The output doesn't make sense since the network is just randomly initialized."
msgstr "ネットワークはランダムに初期化されるので、この時点では出力は意味をなしません。"

#: ../../python/tutorial/by_examples.rst:207
msgid "Backward propagation through the graph"
msgstr "グラフによるバックプロパゲーション"

#: ../../python/tutorial/by_examples.rst:209
msgid "The parameters registered by ``parameter_scope`` management function can be queried by ``get_parameters()`` as a dict format."
msgstr ""
"``parameter_scope`` に登録されることによって管理されるパラメータはディクショナリとして ``get_parameters()`` によってクエリできます。"

#: ../../python/tutorial/by_examples.rst:212
msgid "print(nn.get_parameters())"
msgstr "print(nn.get_parameters())"

#: ../../python/tutorial/by_examples.rst:219
msgid ""
"OrderedDict([('affine1/affine/W', <Variable((64, 10), need_grad=True) at 0x7fa0ba361d50>), ('affine1/affine/b', <Variable((10,), "
"need_grad=True) at 0x7fa0ba361ce8>)])"
msgstr ""
"OrderedDict([('affine1/affine/W', <Variable((64, 10), need_grad=True) at 0x7fa0ba361d50>), ('affine1/affine/b', <Variable((10,), "
"need_grad=True) at 0x7fa0ba361ce8>)])"

#: ../../python/tutorial/by_examples.rst:222
msgid "Before executing backpropagation, we should initialize gradient buffers of all parameter to zeros."
msgstr "バックプロパゲーションを実行する前に、すべてのパラメータの勾配をゼロに初期化しておきます。"

#: ../../python/tutorial/by_examples.rst:225
msgid ""
"​​​for param in nn.get_parameters().values():\n"
"    param.grad.zero()"
msgstr ""

#: ../../python/tutorial/by_examples.rst:230
msgid "Then, you can execute backprop by calling ``backward()`` method at the sink variable."
msgstr "そうすると、グラフの終端の変数で ``backward()`` メソッドを呼び出すことでバックプロパゲーションを実行することができます。"

#: ../../python/tutorial/by_examples.rst:233
msgid ""
"# Compute backward\n"
"loss.backward()\n"
"# Showing gradients.\n"
"for name, param in nn.get_parameters().items():\n"
"    print(name, param.shape, param.g.flat[:20])  # Showing first 20."
msgstr ""
"# Compute backward\n"
"loss.backward()\n"
"# Showing gradients.\n"
"for name, param in nn.get_parameters().items():\n"
"    print(name, param.shape, param.g.flat[:20])  # Showing first 20."

#: ../../python/tutorial/by_examples.rst:244
msgid ""
"affine1/affine/W (64, 10) [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n"
"   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n"
"   0.00000000e+00   0.00000000e+00   4.98418584e-02   8.72317329e-03\n"
"  -4.06671129e-02  -4.68742661e-02   2.52632981e-09   7.86017510e-04\n"
"  9.06870365e-02  -1.56249944e-02  -1.56217301e-02  -3.12499963e-02]\n"
"affine1/affine/b (10,) [ 0.42710391 -0.01852455  0.07369987 -0.04687012 -0.07798236 -0.03664626\n"
"  0.01651323 -0.1249291  -0.11862005 -0.09374455]"
msgstr ""

#: ../../python/tutorial/by_examples.rst:253
msgid "Gradient is stored in grad field of ``Variable``. ``.g`` accessor can be used to access grad data in ``numpy.ndarray`` format."
msgstr ""
"勾配は ``Variable`` の grad フィールドに格納されます。 ``.g`` アクセサは ``numpy.ndarray`` フォーマットで grad フィールドにアクセスする"
"ために使うことができます。"

#: ../../python/tutorial/by_examples.rst:257
msgid "Optimizing parameters (=Training)"
msgstr "パラメータの最適化 ( = ニューラルネットワークの学習 )"

#: ../../python/tutorial/by_examples.rst:259
msgid ""
"To optimize parameters, we provide solver module (aliased as S here). The solver module contains a bunch of optimizer implementations "
"such as SGD, SGD with momentum, Adam etc. The below block creates SGD solver and sets parameters of logistic regression to it."
msgstr ""
"パラメータを最適化するために、( ここでは S と名付けられた ) solver モジュールを用意しています。solver モジュールには、SGD 、モメンタム項"
"をもつ SGD 、 Adam など、たくさんの最適化手法の実装が含まれています。以下のブロックは SGD solver を作り、ロジスティック回帰に用いるパラ"
"メータをそれにセットします。"

#: ../../python/tutorial/by_examples.rst:264
msgid ""
"# Create a solver (gradient-based optimizer)\n"
"learning_rate = 1e-3\n"
"solver = S.Sgd(learning_rate)\n"
"solver.set_parameters(nn.get_parameters())  # Set parameter variables to be updated."
msgstr ""
"# Create a solver (gradient-based optimizer)\n"
"learning_rate = 1e-3\n"
"solver = S.Sgd(learning_rate)\n"
"solver.set_parameters(nn.get_parameters())  # Set parameter variables to be updated."

#: ../../python/tutorial/by_examples.rst:271
msgid ""
"In the next block, we demonstrate a single step of optimization loop. ``solver.zero_grad()`` line does equivalent to calling ``.grad."
"zero()`` for all parameters as we shown above. After backward computation, we apply weight decay, then applying gradient descent "
"implemented in Sgd solver class as follows"
msgstr ""
"次のブロックでは、最適化ループの 中で行われる 1 回分のステップを示しています。``solver.zero_grad()`` は、上記で示したようにすべてのパラ"
"メータに対して ``.grad.zero()`` を呼び出すことと同じです。 バックプロパゲーションを実行したあとで、SGD solver クラスで実装されている重み"
"減衰 ( Weight Decay ) を適用しています。そして最後に、勾配降下法によってパラメータを更新します。数式としては以下のようになります。"

#: ../../python/tutorial/by_examples.rst:277
msgid "\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} L(\\theta, X_{\\mathrm minibatch})"
msgstr "\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} L(\\theta, X_{\\mathrm minibatch})"

#: ../../python/tutorial/by_examples.rst:282
msgid "where :math:`\\eta` denotes learning rate."
msgstr "このとき :math:`\\eta` は学習率を表しています。"

#: ../../python/tutorial/by_examples.rst:284
msgid ""
"# One step of training\n"
"x.d, t.d = data.next()\n"
"loss.forward()\n"
"solver.zero_grad()  # Initialize gradients of all parameters to zero.\n"
"loss.backward()\n"
"solver.weight_decay(1e-5)  # Applying weight decay as an regularization\n"
"solver.update()\n"
"print(loss.d)"
msgstr ""
"# One step of training\n"
"x.d, t.d = data.next()\n"
"loss.forward()\n"
"solver.zero_grad()  # Initialize gradients of all parameters to zero.\n"
"loss.backward()\n"
"solver.weight_decay(1e-5)  # Applying weight decay as an regularization\n"
"solver.update()\n"
"print(loss.d)"

#: ../../python/tutorial/by_examples.rst:298
msgid "12.9438686371"
msgstr "12.9438686371"

#: ../../python/tutorial/by_examples.rst:301
msgid "Next block iterates optimization steps, and shows the loss decreases."
msgstr "次のブロックは最適化ステップをループによって繰り返しています。これによりロスが減少していくのが分かります。"

#: ../../python/tutorial/by_examples.rst:303
msgid ""
"for i in range(1000):\n"
"    x.d, t.d = data.next()\n"
"    loss.forward()\n"
"    solver.zero_grad()  # Initialize gradients of all parameters to zero.\n"
"    loss.backward()\n"
"    solver.weight_decay(1e-5)  # Applying weight decay as an regularization\n"
"    solver.update()\n"
"    if i % 100 == 0:  # Print for each 10 iterations\n"
"        print(i, loss.d)"
msgstr ""
"for i in range(1000):\n"
"    x.d, t.d = data.next()\n"
"    loss.forward()\n"
"    solver.zero_grad()  # Initialize gradients of all parameters to zero.\n"
"    loss.backward()\n"
"    solver.weight_decay(1e-5)  # Applying weight decay as an regularization\n"
"    solver.update()\n"
"    if i % 100 == 0:  # Print for each 10 iterations\n"
"        print(i, loss.d)"

#: ../../python/tutorial/by_examples.rst:318
msgid ""
"0 12.6905069351\n"
"100 3.17041015625\n"
"200 1.60036706924\n"
"300 0.673069953918\n"
"400 0.951370298862\n"
"500 0.724424362183\n"
"600 0.361597299576\n"
"700 0.588107347488\n"
"800 0.28792989254\n"
"900 0.415006935596"
msgstr ""
"0 12.6905069351 \n"
"100 3.17041015625 \n"
"200 1.60036706924 \n"
"300 0.673069953918 \n"
"400 0.951370298862 \n"
"500 0.724424362183 \n"
"600 0.361597299576 \n"
"700 0.588107347488 \n"
"800 0.28792989254 \n"
"900 0.415006935596"

#: ../../python/tutorial/by_examples.rst:331
msgid "Show prediction"
msgstr "予測"

#: ../../python/tutorial/by_examples.rst:333
msgid "The following code displays training results."
msgstr "次のコードは学習の結果を示しています。"

#: ../../python/tutorial/by_examples.rst:335
msgid ""
"x.d, t.d = data.next()  # Here we predict images from training set although it's useless.\n"
"y.forward()  # You can execute a sub graph.\n"
"plt.imshow(tile_images(x.d), **imshow_opt)\n"
"print(\"prediction:\")\n"
"print(y.d.argmax(axis=1).reshape(8, 8))  # Taking a class index based on prediction score."
msgstr ""
"x.d, t.d = data.next()  # Here we predict images from training set although it's useless.\n"
"y.forward()  # You can execute a sub graph.\n"
"plt.imshow(tile_images(x.d), **imshow_opt)\n"
"print(\"prediction:\")\n"
"print(y.d.argmax(axis=1).reshape(8, 8))  # Taking a class index based on prediction score."

#: ../../python/tutorial/by_examples.rst:346
msgid ""
"prediction:\n"
"[[5 0 1 9 0 1 3 3]\n"
" [2 4 1 7 4 5 6 5]\n"
" [7 7 9 7 9 0 7 3]\n"
" [5 3 7 6 6 8 0 9]\n"
" [0 1 3 5 5 5 4 9]\n"
" [1 0 0 8 5 1 8 8]\n"
" [7 5 0 7 6 9 0 0]\n"
" [0 6 2 6 4 4 2 6]]"
msgstr ""
"prediction: \n"
"[[5 0 1 9 0 1 3 3] \n"
" [2 4 1 7 4 5 6 5] \n"
" [7 7 9 7 9 0 7 3] \n"
" [5 3 7 6 6 8 0 9] \n"
" [0 1 3 5 5 5 4 9] \n"
" [1 0 0 8 5 1 8 8] \n"
" [7 5 0 7 6 9 0 0] \n"
" [0 6 2 6 4 4 2 6]]"

#: ../../python/tutorial/by_examples.rst:362
msgid "Dynamic graph construction support"
msgstr "動的グラフのサポート"

#: ../../python/tutorial/by_examples.rst:364
msgid ""
"This is another way of running computation graph in NNabla. This example doesn't show how useful dynamic graph is, but shows a bit of "
"flavor."
msgstr ""
"これは NNabla で計算グラフを実行するもう一つの方法です。この例では動的グラフの有用性を示しきれていませんが、その片鱗を伝えることができれ"
"ばと思います。"

#: ../../python/tutorial/by_examples.rst:367
msgid "The next block just define computation graph building as functions for later use."
msgstr "次のブロックでは、後に使うために計算グラフを構築するための関数を定義しています。"

#: ../../python/tutorial/by_examples.rst:370
msgid ""
"def logreg_forward(x):\n"
"    with nn.parameter_scope(\"affine1\"):\n"
"        y = PF.affine(x, 10)\n"
"    return y\n"
"\n"
"def logreg_loss(y, t):\n"
"    loss = F.mean(F.softmax_cross_entropy(y, t))  # Softmax Xentropy fits multi-class classification problems\n"
"    return loss"
msgstr ""
"def logreg_forward(x):\n"
"    with nn.parameter_scope(\"affine1\"):\n"
"        y = PF.affine(x, 10)\n"
"    return y\n"
"\n"
"def logreg_loss(y, t):\n"
"    loss = F.mean(F.softmax_cross_entropy(y, t))  # Softmax Xentropy fits multi-class classification problems\n"
"    return loss"

#: ../../python/tutorial/by_examples.rst:381
msgid ""
"To run a computation graph dynamically during creation, you use ``nnabla.auto_forward()`` context as you see in the below block. By "
"this, computation is fired immediately at functions are called. (You can also use ``nnabla.set_auto_forward(auto)`` to set the auto-"
"forward state globally.)"
msgstr ""
"計算グラフの作成と同時にそのグラフで定義された計算を実行するためには、下記のブロックで示すように ``nnabla.auto_forward()`` コンテキスト"
"を使う必要があります。これにより、グラフ内で各演算を行う関数が呼び出された瞬間にその演算が実行されます。 ( また、グローバルで auto-"
"forward 状態にするために ``nnabla.set_auto_forward(auto)`` を使うこともできます。 )"

#: ../../python/tutorial/by_examples.rst:387
msgid ""
"x = nn.Variable(img.shape)\n"
"t = nn.Variable(label.shape)\n"
"x.d, t.d = data.next()\n"
"with nn.auto_forward():  # Graph are executed\n"
"    y = logreg_forward(x)\n"
"    loss = logreg_loss(y, t)\n"
"print(\"Loss: {}\".format(loss.d))\n"
"plt.imshow(tile_images(x.d), **imshow_opt)\n"
"print(\"prediction:\")\n"
"print(y.d.argmax(axis=1).reshape(8, 8))"
msgstr ""
"x = nn.Variable(img.shape)\n"
"t = nn.Variable(label.shape)\n"
"x.d, t.d = data.next()\n"
"with nn.auto_forward():  # Graph are executed\n"
"    y = logreg_forward(x)\n"
"    loss = logreg_loss(y, t)\n"
"print(\"Loss: {}\".format(loss.d))\n"
"plt.imshow(tile_images(x.d), **imshow_opt)\n"
"print(\"prediction:\")\n"
"print(y.d.argmax(axis=1).reshape(8, 8))"

#: ../../python/tutorial/by_examples.rst:403
msgid ""
"Loss: 0.43071603775\n"
"prediction:\n"
"[[9 3 5 0 1 9 9 2]\n"
" [5 6 6 2 7 5 1 1]\n"
" [3 7 7 6 0 8 3 8]\n"
" [0 6 4 6 0 6 9 9]\n"
" [6 1 2 5 8 3 2 4]\n"
" [1 4 4 0 5 7 1 7]\n"
" [7 8 9 5 8 3 7 8]\n"
" [5 7 5 3 3 0 0 7]]"
msgstr ""
"Loss: 0.43071603775 \n"
"prediction: \n"
"[[9 3 5 0 1 9 9 2] \n"
" [5 6 6 2 7 5 1 1] \n"
" [3 7 7 6 0 8 3 8] \n"
" [0 6 4 6 0 6 9 9] \n"
" [6 1 2 5 8 3 2 4] \n"
" [1 4 4 0 5 7 1 7] \n"
" [7 8 9 5 8 3 7 8] \n"
" [5 7 5 3 3 0 0 7]]"

#: ../../python/tutorial/by_examples.rst:419
msgid "Backward computation can be done on a dynamically constructed graph."
msgstr "バックプロパゲーションは動的に作られたグラフでも実行できます。"

#: ../../python/tutorial/by_examples.rst:421
msgid ""
"solver.zero_grad()\n"
"loss.backward()"
msgstr ""
"solver.zero_grad() \n"
"loss.backward()"

#: ../../python/tutorial/by_examples.rst:427
msgid "Multi-Layer Perceptron (MLP)"
msgstr "多層パーセプトロン ( MLP )"

#: ../../python/tutorial/by_examples.rst:429
msgid "In this section, you see an example of MLP graph building and training."
msgstr "このセクションでは、多層パーセプトロン ( MLP ) を表す計算グラフの構築、およびその学習を行う例を見ていきます。"

#: ../../python/tutorial/by_examples.rst:431
msgid "Before starting, we clear all parameters registered in the logistic regression example."
msgstr "始める前に、ロジスティック回帰のサンプルで登録したすべてのパラメータを消去します。"

#: ../../python/tutorial/by_examples.rst:434
msgid "nn.clear_parameters()  # Clear all parameters"
msgstr "nn.clear_parameters()  # Clear all parameters"

#: ../../python/tutorial/by_examples.rst:438
msgid "Here is the function that builds a MLP with an arbitrary depth and width for 10 class classification."
msgstr "10 クラスの分類問題を解くために、任意の深さと幅をもつ MLP を構築する関数を定義します。"

#: ../../python/tutorial/by_examples.rst:441
msgid ""
"def mlp(x, hidden=[16, 32, 16]):\n"
"    hs = []\n"
"    with nn.parameter_scope(\"mlp\"):  # Parameter scope can be nested\n"
"        h = x\n"
"        for hid, hsize in enumerate(hidden):\n"
"            with nn.parameter_scope(\"affine{}\".format(hid + 1)):\n"
"                h = F.tanh(PF.affine(h, hsize))\n"
"                hs.append(h)\n"
"        with nn.parameter_scope(\"classifier\"):\n"
"            y = PF.affine(h, 10)\n"
"    return y, hs"
msgstr ""
"def mlp(x, hidden=[16, 32, 16]):\n"
"    hs = []\n"
"    with nn.parameter_scope(\"mlp\"):  # Parameter scope can be nested\n"
"        h = x\n"
"        for hid, hsize in enumerate(hidden):\n"
"            with nn.parameter_scope(\"affine{}\".format(hid + 1)):\n"
"                h = F.tanh(PF.affine(h, hsize))\n"
"                hs.append(h)\n"
"        with nn.parameter_scope(\"classifier\"):\n"
"            y = PF.affine(h, 10)\n"
"    return y, hs"

#: ../../python/tutorial/by_examples.rst:455
msgid ""
"# Construct a MLP graph\n"
"y, hs = mlp(x)"
msgstr ""
"# Construct a MLP graph\n"
"y, hs = mlp(x)"

#: ../../python/tutorial/by_examples.rst:460
msgid ""
"print(\"Printing shapes\")\n"
"print(\"x: {}\".format(x.shape))\n"
"for i, h in enumerate(hs):\n"
"    print(\"h{}:\".format(i + 1), h.shape)\n"
"print(\"y: {}\".format(y.shape))"
msgstr ""
"print(\"Printing shapes\") \n"
"print(\"x: {}\".format(x.shape)) \n"
"for i, h in enumerate(hs): \n"
"    print(\"h{}:\".format(i + 1), h.shape) \n"
"print(\"y: {}\".format(y.shape))"

#: ../../python/tutorial/by_examples.rst:471
msgid ""
"Printing shapes\n"
"x: (64, 1, 8, 8)\n"
"h2: (64, 16)\n"
"h2: (64, 32)\n"
"h3: (64, 16)\n"
"y: (64, 10)"
msgstr ""

#: ../../python/tutorial/by_examples.rst:479
msgid ""
"# Training\n"
"loss = logreg_loss(y, t)  # Reuse logreg loss function.\n"
"\n"
"# Copied from the above logreg example.\n"
"def training(steps, learning_rate):\n"
"    solver = S.Sgd(learning_rate)\n"
"    solver.set_parameters(nn.get_parameters())  # Set parameter variables to be updated.\n"
"    for i in range(steps):\n"
"        x.d, t.d = data.next()\n"
"        loss.forward()\n"
"        solver.zero_grad()  # Initialize gradients of all parameters to zero.\n"
"        loss.backward()\n"
"        solver.weight_decay(1e-5)  # Applying weight decay as an regularization\n"
"        solver.update()\n"
"        if i % 100 == 0:  # Print for each 10 iterations\n"
"            print(i, loss.d)\n"
"\n"
"\n"
"# Training\n"
"training(1000, 1e-2)"
msgstr ""
"# Training\n"
"loss = logreg_loss(y, t)  # Reuse logreg loss function.\n"
"\n"
"# Copied from the above logreg example.\n"
"def training(steps, learning_rate):\n"
"    solver = S.Sgd(learning_rate)\n"
"    solver.set_parameters(nn.get_parameters())  # Set parameter variables to be updated.\n"
"    for i in range(steps):\n"
"        x.d, t.d = data.next()\n"
"        loss.forward()\n"
"        solver.zero_grad()  # Initialize gradients of all parameters to zero.\n"
"        loss.backward()\n"
"        solver.weight_decay(1e-5)  # Applying weight decay as an regularization\n"
"        solver.update()\n"
"        if i % 100 == 0:  # Print for each 10 iterations\n"
"            print(i, loss.d)\n"
"\n"
"\n"
"# Training\n"
"training(1000, 1e-2)"

#: ../../python/tutorial/by_examples.rst:505
msgid ""
"0 2.42193937302\n"
"100 1.83251476288\n"
"200 1.49943637848\n"
"300 1.30751883984\n"
"400 1.00974023342\n"
"500 0.904026031494\n"
"600 0.873289525509\n"
"700 0.725554704666\n"
"800 0.614291608334\n"
"900 0.555113613605"
msgstr ""
"0 2.42193937302 \n"
"100 1.83251476288 \n"
"200 1.49943637848 \n"
"300 1.30751883984 \n"
"400 1.00974023342 \n"
"500 0.904026031494 \n"
"600 0.873289525509 \n"
"700 0.725554704666 \n"
"800 0.614291608334 \n"
"900 0.555113613605"

#: ../../python/tutorial/by_examples.rst:517
msgid ""
"# Showing responses for each layer\n"
"num_plot = len(hs) + 2\n"
"gid = 1\n"
"\n"
"def scale01(h):\n"
"    return (h - h.min()) / (h.max() - h.min())\n"
"\n"
"def imshow(img, title):\n"
"    global gid\n"
"    plt.subplot(num_plot, 1, gid)\n"
"    gid += 1\n"
"    plt.title(title)\n"
"    plt.imshow(img, **imshow_opt)\n"
"    plt.axis('off')\n"
"\n"
"plt.figure(figsize=(2, 5))\n"
"imshow(x.d[0, 0], 'x')\n"
"for hid, h in enumerate(hs):\n"
"    imshow(scale01(h.d[0]).reshape(-1, 8), 'h{}'.format(hid + 1))\n"
"imshow(scale01(y.d[0]).reshape(2, 5), 'y')"
msgstr ""
"# Showing responses for each layer\n"
"num_plot = len(hs) + 2\n"
"gid = 1\n"
"\n"
"def scale01(h):\n"
"    return (h - h.min()) / (h.max() - h.min())\n"
"\n"
"def imshow(img, title):\n"
"    global gid\n"
"    plt.subplot(num_plot, 1, gid)\n"
"    gid += 1\n"
"    plt.title(title)\n"
"    plt.imshow(img, **imshow_opt)\n"
"    plt.axis('off')\n"
"\n"
"plt.figure(figsize=(2, 5))\n"
"imshow(x.d[0, 0], 'x')\n"
"for hid, h in enumerate(hs):\n"
"    imshow(scale01(h.d[0]).reshape(-1, 8), 'h{}'.format(hid + 1))\n"
"imshow(scale01(y.d[0]).reshape(2, 5), 'y')"

#: ../../python/tutorial/by_examples.rst:546
msgid "Convolutional Neural Network with CUDA acceleration"
msgstr "CUDA による高速な畳み込みニューラルネットワークの実行"

#: ../../python/tutorial/by_examples.rst:548
msgid "Here we demonstrates a CNN with CUDA GPU acceleration."
msgstr "ここでは、 CUDA を利用できる GPU を用いた CNN の高速な実行例を見ていきます。"

#: ../../python/tutorial/by_examples.rst:550 ../../python/tutorial/by_examples.rst:667
msgid "nn.clear_parameters()"
msgstr "nn.clear_parameters()"

#: ../../python/tutorial/by_examples.rst:554
msgid ""
"def cnn(x):\n"
"    with nn.parameter_scope(\"cnn\"):  # Parameter scope can be nested\n"
"        with nn.parameter_scope(\"conv1\"):\n"
"            c1 = F.tanh(PF.batch_normalization(\n"
"                PF.convolution(x, 4, (3, 3), pad=(1, 1), stride=(2, 2))))\n"
"        with nn.parameter_scope(\"conv2\"):\n"
"            c2 = F.tanh(PF.batch_normalization(\n"
"                PF.convolution(c1, 8, (3, 3), pad=(1, 1))))\n"
"            c2 = F.average_pooling(c2, (2, 2))\n"
"        with nn.parameter_scope(\"fc3\"):\n"
"            fc3 = F.tanh(PF.affine(c2, 32))\n"
"        with nn.parameter_scope(\"classifier\"):\n"
"            y = PF.affine(fc3, 10)\n"
"    return y, [c1, c2, fc3]"
msgstr ""
"def cnn(x):\n"
"    with nn.parameter_scope(\"cnn\"):  # Parameter scope can be nested\n"
"        with nn.parameter_scope(\"conv1\"):\n"
"            c1 = F.tanh(PF.batch_normalization(\n"
"                PF.convolution(x, 4, (3, 3), pad=(1, 1), stride=(2, 2))))\n"
"        with nn.parameter_scope(\"conv2\"):\n"
"            c2 = F.tanh(PF.batch_normalization(\n"
"                PF.convolution(c1, 8, (3, 3), pad=(1, 1))))\n"
"            c2 = F.average_pooling(c2, (2, 2))\n"
"        with nn.parameter_scope(\"fc3\"):\n"
"            fc3 = F.tanh(PF.affine(c2, 32))\n"
"        with nn.parameter_scope(\"classifier\"):\n"
"            y = PF.affine(fc3, 10)\n"
"    return y, [c1, c2, fc3]"

#: ../../python/tutorial/by_examples.rst:571
msgid ""
"To enable CUDA extension in NNabla, you have to install nnabla-ext-cuda package first. See `the install guide <http://nnabla."
"readthedocs.io/en/latest/python/installation.html>`__. After installing the CUDA extension, you can easily switch to run on CUDA by "
"specifying a context before building a graph. We strongly recommend using a cuDNN context that is fast. Although the context class can "
"be instantiated by ``nn.Context()``, specifying a context descriptor might be a bit complicated for users. There for we recommend "
"create a context by using a helper function ``get_extension_context()`` found in the ``nnabla.ext_utils`` module. NNabla officially "
"supports ``cpu`` and ``cudnn`` as a context specifier passed to the first argument (extension name). NOTE: By setting the cudnn context "
"as a global default context, Functions and solves created are instantiated with cuDNN (preferred) mode. You can also specify a context "
"using ``with nn.context_scope()``. See `API reference <http://nnabla.readthedocs.io/en/latest/python/api/common.html#context>`__ for "
"details."
msgstr ""
"NNabla で CUDA を利用するためには、まず、 nnabla-ext-cuda パッケージをインストールする必要があります。 `インストールガイド <http://"
"nnabla.readthedocs.io/en/latest/python/installation.html>`__ を参照してください。CUDA エクステンションをインストールしたあと、グラフを作"
"成する前にコンテキストを指定することによって、簡単に CUDA での実行に切り替えることができます。 cuDNN コンテキストは計算の高速実行が可能"
"であることから、使用を強く推奨します。 コンテキストのクラスは ``nn.Context()`` によってインスタンス化できますが、コンテキストの記述子を"
"指定することはユーザーにとって少し複雑かもしれません。そこで、 ``nnabla.ext_utils`` モジュールにある helper 関数 "
"``get_extension_context()`` を使ってコンテキストを作ることを推奨します。 NNabla は最初の引数 ( 拡張名 ) に渡されるコンテキストの指示子と"
"して ``cpu`` や ``cudnn`` を公式にサポートしています。\n"
"注 : cuDNN コンテキストを全体的なデフォルトのコンテキストとしてセットすることによって、作成された関数や solver は cuDNN モード (より好ま"
"しい ) でインスタンスが作られます。また、 ``nn.context_scope()`` を使うことでコンテキストを指定することができます。詳細は `API リファレ"
"ンス <http://nnabla.readthedocs.io/jp/latest/python/api/common.html#context>`__ を参照してください。"

#: ../../python/tutorial/by_examples.rst:589
msgid ""
"# Run on CUDA\n"
"from nnabla.ext_utils import get_extension_context\n"
"cuda_device_id = 0\n"
"ctx = get_extension_context('cudnn', device_id=cuda_device_id)\n"
"print(\"Context: {}\".format(ctx))\n"
"nn.set_default_context(ctx)  # Set CUDA as a default context.\n"
"y, hs = cnn(x)\n"
"loss = logreg_loss(y, t)"
msgstr ""
"# Run on CUDA\n"
"from nnabla.ext_utils import get_extension_context\n"
"cuda_device_id = 0\n"
"ctx = get_extension_context('cudnn', device_id=cuda_device_id)\n"
"print(\"Context: {}\".format(ctx))\n"
"nn.set_default_context(ctx)  # Set CUDA as a default context.\n"
"y, hs = cnn(x)\n"
"loss = logreg_loss(y, t)"

#: ../../python/tutorial/by_examples.rst:603
msgid ""
"2017-06-26 23:09:54,555 [nnabla][INFO]: Initializing CUDA extension...\n"
"2017-06-26 23:09:54,731 [nnabla][INFO]: Initializing cuDNN extension..."
msgstr ""
"2017-06-26 23:09:54,555 [nnabla][INFO]: Initializing CUDA extension... \n"
"2017-06-26 23:09:54,731 [nnabla][INFO]: Initializing cuDNN extension..."

#: ../../python/tutorial/by_examples.rst:609
msgid "Context: Context(backend='cpu|cuda', array_class='CudaCachedArray', device_id='0', compute_backend='default|cudnn')"
msgstr "Context: Context(backend='cpu|cuda', array_class='CudaCachedArray', device_id='0', compute_backend='default|cudnn')"

#: ../../python/tutorial/by_examples.rst:612
msgid "training(1000, 1e-1)"
msgstr "training(1000, 1e-1)"

#: ../../python/tutorial/by_examples.rst:619
msgid ""
"0 2.34862923622\n"
"100 1.00527024269\n"
"200 0.416576713324\n"
"300 0.240603536367\n"
"400 0.254562884569\n"
"500 0.206138283014\n"
"600 0.220851421356\n"
"700 0.161689639091\n"
"800 0.230873346329\n"
"900 0.121101222932"
msgstr ""
"0 2.34862923622 \n"
"100 1.00527024269 \n"
"200 0.416576713324 \n"
"300 0.240603536367 \n"
"400 0.254562884569 \n"
"500 0.206138283014 \n"
"600 0.220851421356 \n"
"700 0.161689639091 \n"
"800 0.230873346329 \n"
"900 0.121101222932"

#: ../../python/tutorial/by_examples.rst:631
msgid ""
"# Showing responses for each layer\n"
"num_plot = len(hs) + 2\n"
"gid = 1\n"
"plt.figure(figsize=(2, 8))\n"
"imshow(x.d[0, 0], 'x')\n"
"imshow(tile_images(hs[0].d[0][:, None]), 'conv1')\n"
"imshow(tile_images(hs[1].d[0][:, None]), 'conv2')\n"
"imshow(hs[2].d[0].reshape(-1, 8), 'fc3')\n"
"imshow(scale01(y.d[0]).reshape(2, 5), 'y')"
msgstr ""
"# Showing responses for each layer\n"
"num_plot = len(hs) + 2\n"
"gid = 1\n"
"plt.figure(figsize=(2, 8))\n"
"imshow(x.d[0, 0], 'x')\n"
"imshow(tile_images(hs[0].d[0][:, None]), 'conv1')\n"
"imshow(tile_images(hs[1].d[0][:, None]), 'conv2')\n"
"imshow(hs[2].d[0].reshape(-1, 8), 'fc3')\n"
"imshow(scale01(y.d[0]).reshape(2, 5), 'y')"

#: ../../python/tutorial/by_examples.rst:648
msgid "``nn.save_parameters`` writes parameters registered in ``parameter_scope`` system in HDF5 format. We use it a later example."
msgstr ""
"``nn.save_parameters`` は HDF5 フォーマットで ``parameter_scope`` に登録されたパラメータを書き出します。それは後述のサンプルで使用しま"
"す。"

#: ../../python/tutorial/by_examples.rst:651
msgid ""
"path_cnn_params = \"tmp.params.cnn.h5\"\n"
"nn.save_parameters(path_cnn_params)"
msgstr ""
"path_cnn_params = \"tmp.params.cnn.h5\" \n"
"nn.save_parameters(path_cnn_params)"

#: ../../python/tutorial/by_examples.rst:659
msgid "2017-06-26 23:09:56,132 [nnabla][INFO]: Parameter save (hdf5): tmp.params.cnn.h5"
msgstr "2017-06-26 23:09:56,132 [nnabla][INFO]: Parameter save (hdf5): tmp.params.cnn.h5"

#: ../../python/tutorial/by_examples.rst:663
msgid "Recurrent Neural Network (Elman RNN)"
msgstr "回帰型ニューラルネットワーク ( Elman RNN )"

#: ../../python/tutorial/by_examples.rst:665
msgid "This is an example of recurrent neural network training."
msgstr "これは回帰型ニューラルネットワークの学習のサンプルです。"

#: ../../python/tutorial/by_examples.rst:671
msgid ""
"def rnn(xs, h0, hidden=32):\n"
"    hs = []\n"
"    with nn.parameter_scope(\"rnn\"):\n"
"        h = h0\n"
"        # Time step loop\n"
"        for x in xs:\n"
"            # Note: Parameter scopes are reused over time\n"
"            # which means parameters are shared over time.\n"
"            with nn.parameter_scope(\"x2h\"):\n"
"                x2h = PF.affine(x, hidden, with_bias=False)\n"
"            with nn.parameter_scope(\"h2h\"):\n"
"                h2h = PF.affine(h, hidden)\n"
"            h = F.tanh(x2h + h2h)\n"
"            hs.append(h)\n"
"        with nn.parameter_scope(\"classifier\"):\n"
"            y = PF.affine(h, 10)\n"
"    return y, hs"
msgstr ""
"def rnn(xs, h0, hidden=32):\n"
"    hs = []\n"
"    with nn.parameter_scope(\"rnn\"):\n"
"        h = h0\n"
"        # Time step loop\n"
"        for x in xs:\n"
"            # Note: Parameter scopes are reused over time\n"
"            # which means parameters are shared over time.\n"
"            with nn.parameter_scope(\"x2h\"):\n"
"                x2h = PF.affine(x, hidden, with_bias=False)\n"
"            with nn.parameter_scope(\"h2h\"):\n"
"                h2h = PF.affine(h, hidden)\n"
"            h = F.tanh(x2h + h2h)\n"
"            hs.append(h)\n"
"        with nn.parameter_scope(\"classifier\"):\n"
"            y = PF.affine(h, 10)\n"
"    return y, hs"

#: ../../python/tutorial/by_examples.rst:691
msgid "It is not meaningful, but just a demonstration purpose. We split an image into 2 by 2 grids, and feed them sequentially into RNN."
msgstr "この操作には特に意味はなく、デモを意図しています。画像を 2 × 2 のグリッドに分けて、それらを連続的に RNN に与えます。"

#: ../../python/tutorial/by_examples.rst:694
msgid ""
"def split_grid4(x):\n"
"    x0 = x[..., :4, :4]\n"
"    x1 = x[..., :4, 4:]\n"
"    x2 = x[..., 4:, :4]\n"
"    x3 = x[..., 4:, 4:]\n"
"    return x0, x1, x2, x3"
msgstr ""
"def split_grid4(x):\n"
"    x0 = x[..., :4, :4]\n"
"    x1 = x[..., :4, 4:]\n"
"    x2 = x[..., 4:, :4]\n"
"    x3 = x[..., 4:, 4:]\n"
"    return x0, x1, x2, x3"

#: ../../python/tutorial/by_examples.rst:703
msgid ""
"hidden = 32\n"
"seq_img = split_grid4(img)\n"
"seq_x = [nn.Variable(subimg.shape) for subimg in seq_img]\n"
"h0 = nn.Variable((img.shape[0], hidden))  # Initial hidden state.\n"
"y, hs = rnn(seq_x, h0, hidden)\n"
"loss = logreg_loss(y, t)"
msgstr ""
"hidden = 32\n"
"seq_img = split_grid4(img)\n"
"seq_x = [nn.Variable(subimg.shape) for subimg in seq_img]\n"
"h0 = nn.Variable((img.shape[0], hidden))  # Initial hidden state.\n"
"y, hs = rnn(seq_x, h0, hidden)\n"
"loss = logreg_loss(y, t)"

#: ../../python/tutorial/by_examples.rst:712
msgid ""
"# Copied from the above logreg example.\n"
"def training_rnn(steps, learning_rate):\n"
"    solver = S.Sgd(learning_rate)\n"
"    solver.set_parameters(nn.get_parameters())  # Set parameter variables to be updated.\n"
"    for i in range(steps):\n"
"        minibatch = data.next()\n"
"        img, t.d = minibatch\n"
"        seq_img = split_grid4(img)\n"
"        h0.d = 0  # Initialize as 0\n"
"        for x, subimg in zip(seq_x, seq_img):\n"
"            x.d = subimg\n"
"        loss.forward()\n"
"        solver.zero_grad()  # Initialize gradients of all parameters to zero.\n"
"        loss.backward()\n"
"        solver.weight_decay(1e-5)  # Applying weight decay as an regularization\n"
"        solver.update()\n"
"        if i % 100 == 0:  # Print for each 10 iterations\n"
"            print(i, loss.d)\n"
"\n"
"training_rnn(1000, 1e-1)"
msgstr ""
"# Copied from the above logreg example.\n"
"def training_rnn(steps, learning_rate):\n"
"    solver = S.Sgd(learning_rate)\n"
"    solver.set_parameters(nn.get_parameters())  # Set parameter variables to be updated.\n"
"    for i in range(steps):\n"
"        minibatch = data.next()\n"
"        img, t.d = minibatch\n"
"        seq_img = split_grid4(img)\n"
"        h0.d = 0  # Initialize as 0\n"
"        for x, subimg in zip(seq_x, seq_img):\n"
"            x.d = subimg\n"
"        loss.forward()\n"
"        solver.zero_grad()  # Initialize gradients of all parameters to zero.\n"
"        loss.backward()\n"
"        solver.weight_decay(1e-5)  # Applying weight decay as an regularization\n"
"        solver.update()\n"
"        if i % 100 == 0:  # Print for each 10 iterations\n"
"            print(i, loss.d)\n"
"\n"
"training_rnn(1000, 1e-1)"

#: ../../python/tutorial/by_examples.rst:738
msgid ""
"0 2.62527275085\n"
"100 0.780260562897\n"
"200 0.486522495747\n"
"300 0.289345681667\n"
"400 0.249717146158\n"
"500 0.538961410522\n"
"600 0.276877015829\n"
"700 0.159639537334\n"
"800 0.249660402536\n"
"900 0.0925596579909"
msgstr ""
"0 2.62527275085 \n"
"100 0.780260562897 \n"
"200 0.486522495747 \n"
"300 0.289345681667 \n"
"400 0.249717146158 \n"
"500 0.538961410522 \n"
"600 0.276877015829 \n"
"700 0.159639537334 \n"
"800 0.249660402536 \n"
"900 0.0925596579909"

#: ../../python/tutorial/by_examples.rst:750
msgid ""
"# Showing responses for each layer\n"
"num_plot = len(hs) + 2\n"
"gid = 1\n"
"plt.figure(figsize=(2, 8))\n"
"imshow(x.d[0, 0], 'x')\n"
"for hid, h in enumerate(hs):\n"
"    imshow(scale01(h.d[0]).reshape(-1, 8), 'h{}'.format(hid + 1))\n"
"imshow(scale01(y.d[0]).reshape(2, 5), 'y')"
msgstr ""
"# Showing responses for each layer\n"
"num_plot = len(hs) + 2\n"
"gid = 1\n"
"plt.figure(figsize=(2, 8))\n"
"imshow(x.d[0, 0], 'x')\n"
"for hid, h in enumerate(hs):\n"
"    imshow(scale01(h.d[0]).reshape(-1, 8), 'h{}'.format(hid + 1))\n"
"imshow(scale01(y.d[0]).reshape(2, 5), 'y')"

#: ../../python/tutorial/by_examples.rst:767
msgid "Siamese Network"
msgstr "シャムネットワーク"

#: ../../python/tutorial/by_examples.rst:769
msgid ""
"This example show how to embed an image in a categorical dataset into 2D space using deep learning. This also demonstrates how to reuse "
"a pretrained network."
msgstr ""
"このサンプルは、深層学習を使って 2 次元に、画像を分類別のデータセットに埋め込む方法を示しています。また、事前に学習されたネットワークを"
"再利用する方法も説明しています。"

#: ../../python/tutorial/by_examples.rst:773
msgid "First, we load parameters learned in the CNN example."
msgstr "初めに、 CNN のサンプルで学習されたパラメータをロードします。"

#: ../../python/tutorial/by_examples.rst:775
msgid ""
"nn.clear_parameters()\n"
"# Loading CNN pretrained parameters.\n"
"_ = nn.load_parameters(path_cnn_params)"
msgstr ""
"nn.clear_parameters()\n"
"# Loading CNN pretrained parameters.\n"
"_ = nn.load_parameters(path_cnn_params)"

#: ../../python/tutorial/by_examples.rst:784
msgid "2017-06-26 23:09:57,838 [nnabla][INFO]: Parameter load (<built-in function format>): tmp.params.cnn.h5"
msgstr "2017-06-26 23:09:57,838 [nnabla][INFO]: Parameter load (<built-in function format>): tmp.params.cnn.h5"

#: ../../python/tutorial/by_examples.rst:787
msgid ""
"We define embedding function. Note that the network structure and parameter hierarchy is identical to the previous CNN example. That "
"enables you to reuse the saved parameters and finetune from it."
msgstr ""
"埋め込み関数を定義します。ネットワーク構造やパラメータの階層は以前の CNN のサンプルと同じであることに注意してください。これにより、保存"
"したパラメータを再利用したり、パラメータをファインチューンしたりすることができます。"

#: ../../python/tutorial/by_examples.rst:791
msgid ""
"def cnn_embed(x, test=False):\n"
"    # Note: Identical configuration with the CNN example above.\n"
"    # Parameters pretrained in the above CNN example are used.\n"
"    with nn.parameter_scope(\"cnn\"):\n"
"        with nn.parameter_scope(\"conv1\"):\n"
"            c1 = F.tanh(PF.batch_normalization(PF.convolution(x, 4, (3, 3), pad=(1, 1), stride=(2, 2)), batch_stat=not test))\n"
"        with nn.parameter_scope(\"conv2\"):\n"
"            c2 = F.tanh(PF.batch_normalization(PF.convolution(c1, 8, (3, 3), pad=(1, 1)), batch_stat=not test))\n"
"            c2 = F.average_pooling(c2, (2, 2))\n"
"        with nn.parameter_scope(\"fc3\"):\n"
"            fc3 = PF.affine(c2, 32)\n"
"    # Additional affine for map into 2D.\n"
"    with nn.parameter_scope(\"embed2d\"):\n"
"        embed = PF.affine(c2, 2)\n"
"    return embed, [c1, c2, fc3]\n"
"\n"
"def siamese_loss(e0, e1, t, margin=1.0, eps=1e-4):\n"
"    dist = F.sum(F.squared_error(e0, e1), axis=1)  # Squared distance\n"
"    # Contrastive loss\n"
"    sim_cost = t * dist\n"
"    dissim_cost = (1 - t) * \\\n"
"        (F.maximum_scalar(margin - (dist + eps) ** (0.5), 0) ** 2)\n"
"    return F.mean(sim_cost + dissim_cost)"
msgstr ""
"def cnn_embed(x, test=False):\n"
"    # Note: Identical configuration with the CNN example above.\n"
"    # Parameters pretrained in the above CNN example are used.\n"
"    with nn.parameter_scope(\"cnn\"):\n"
"        with nn.parameter_scope(\"conv1\"):\n"
"            c1 = F.tanh(PF.batch_normalization(PF.convolution(x, 4, (3, 3), pad=(1, 1), stride=(2, 2)), batch_stat=not test))\n"
"        with nn.parameter_scope(\"conv2\"):\n"
"            c2 = F.tanh(PF.batch_normalization(PF.convolution(c1, 8, (3, 3), pad=(1, 1)), batch_stat=not test))\n"
"            c2 = F.average_pooling(c2, (2, 2))\n"
"        with nn.parameter_scope(\"fc3\"):\n"
"            fc3 = PF.affine(c2, 32)\n"
"    # Additional affine for map into 2D.\n"
"    with nn.parameter_scope(\"embed2d\"):\n"
"        embed = PF.affine(c2, 2)\n"
"    return embed, [c1, c2, fc3]\n"
"\n"
"def siamese_loss(e0, e1, t, margin=1.0, eps=1e-4):\n"
"    dist = F.sum(F.squared_error(e0, e1), axis=1)  # Squared distance\n"
"    # Contrastive loss\n"
"    sim_cost = t * dist\n"
"    dissim_cost = (1 - t) * \\\n"
"        (F.maximum_scalar(margin - (dist + eps) ** (0.5), 0) ** 2)\n"
"    return F.mean(sim_cost + dissim_cost)"

#: ../../python/tutorial/by_examples.rst:817
msgid ""
"We build two stream CNNs and compare them with the contrastive loss function defined above. Note that both CNNs have the same parameter "
"hierarchy, which means both parameters are shared."
msgstr ""
"2 つの CNN を作成し、上記で定義したコントラスティブロス関数で 2 つの CNN の出力を比較します。ここで、両方の CNN が同じパラメータの階層構"
"造を有していることに注意してください。これはすなわち、両方のネットワークは利用するパラメータを共有しているということを意味します。"

#: ../../python/tutorial/by_examples.rst:821
msgid ""
"x0 = nn.Variable(img.shape)\n"
"x1 = nn.Variable(img.shape)\n"
"t = nn.Variable((img.shape[0],))  # Same class or not\n"
"e0, hs0 = cnn_embed(x0)\n"
"e1, hs1 = cnn_embed(x1)  # NOTE: parameters are shared\n"
"loss = siamese_loss(e0, e1, t)"
msgstr ""
"x0 = nn.Variable(img.shape)\n"
"x1 = nn.Variable(img.shape)\n"
"t = nn.Variable((img.shape[0],))  # Same class or not\n"
"e0, hs0 = cnn_embed(x0)\n"
"e1, hs1 = cnn_embed(x1)  # NOTE: parameters are shared\n"
"loss = siamese_loss(e0, e1, t)"

#: ../../python/tutorial/by_examples.rst:830
msgid ""
"def training_siamese(steps):\n"
"    for i in range(steps):\n"
"        minibatchs = []\n"
"        for _ in range(2):\n"
"            minibatch = data.next()\n"
"            minibatchs.append((minibatch[0].copy(), minibatch[1].copy()))\n"
"        x0.d, label0 = minibatchs[0]\n"
"        x1.d, label1 = minibatchs[1]\n"
"        t.d = (label0 == label1).astype(np.int).flat\n"
"        loss.forward()\n"
"        solver.zero_grad()  # Initialize gradients of all parameters to zero.\n"
"        loss.backward()\n"
"        solver.weight_decay(1e-5)  # Applying weight decay as an regularization\n"
"        solver.update()\n"
"        if i % 100 == 0:  # Print for each 10 iterations\n"
"            print(i, loss.d)\n"
"learning_rate = 1e-2\n"
"solver = S.Sgd(learning_rate)\n"
"with nn.parameter_scope(\"embed2d\"):\n"
"    # Only 2d embedding affine will be updated.\n"
"    solver.set_parameters(nn.get_parameters())\n"
"training_siamese(2000)\n"
"# Decay learning rate\n"
"solver.set_learning_rate(solver.learning_rate() * 0.1)\n"
"training_siamese(2000)"
msgstr ""
"def training_siamese(steps):\n"
"    for i in range(steps):\n"
"        minibatchs = []\n"
"        for _ in range(2):\n"
"            minibatch = data.next()\n"
"            minibatchs.append((minibatch[0].copy(), minibatch[1].copy()))\n"
"        x0.d, label0 = minibatchs[0]\n"
"        x1.d, label1 = minibatchs[1]\n"
"        t.d = (label0 == label1).astype(np.int).flat\n"
"        loss.forward()\n"
"        solver.zero_grad()  # Initialize gradients of all parameters to zero.\n"
"        loss.backward()\n"
"        solver.weight_decay(1e-5)  # Applying weight decay as an regularization\n"
"        solver.update()\n"
"        if i % 100 == 0:  # Print for each 10 iterations\n"
"            print(i, loss.d)\n"
"learning_rate = 1e-2\n"
"solver = S.Sgd(learning_rate)\n"
"with nn.parameter_scope(\"embed2d\"):\n"
"    # Only 2d embedding affine will be updated.\n"
"    solver.set_parameters(nn.get_parameters())\n"
"training_siamese(2000)\n"
"# Decay learning rate\n"
"solver.set_learning_rate(solver.learning_rate() * 0.1)\n"
"training_siamese(2000)"

#: ../../python/tutorial/by_examples.rst:861
msgid ""
"0 0.150528043509\n"
"100 0.186870157719\n"
"200 0.149316266179\n"
"300 0.207163512707\n"
"400 0.171384960413\n"
"500 0.190256178379\n"
"600 0.138507723808\n"
"700 0.0918073058128\n"
"800 0.159692272544\n"
"900 0.0833697617054\n"
"1000 0.0839115008712\n"
"1100 0.104669973254\n"
"1200 0.0776312947273\n"
"1300 0.114788673818\n"
"1400 0.120309025049\n"
"1500 0.107732802629\n"
"1600 0.070114441216\n"
"1700 0.101728007197\n"
"1800 0.114350572228\n"
"1900 0.118794307113\n"
"0 0.0669310241938\n"
"100 0.0553173273802\n"
"200 0.0829797014594\n"
"300 0.0951051414013\n"
"400 0.128303915262\n"
"500 0.102963000536\n"
"600 0.0910559669137\n"
"700 0.0898950695992\n"
"800 0.119949311018\n"
"900 0.0603067912161\n"
"1000 0.105748720467\n"
"1100 0.108760476112\n"
"1200 0.0820947736502\n"
"1300 0.0971114039421\n"
"1400 0.0836166366935\n"
"1500 0.0899554267526\n"
"1600 0.109069615602\n"
"1700 0.0921652168036\n"
"1800 0.0759357959032\n"
"1900 0.100669950247"
msgstr ""
"0 0.150528043509 \n"
"100 0.186870157719 \n"
"200 0.149316266179 \n"
"300 0.207163512707 \n"
"400 0.171384960413 \n"
"500 0.190256178379 \n"
"600 0.138507723808 \n"
"700 0.0918073058128 \n"
"800 0.159692272544 \n"
"900 0.0833697617054 \n"
"1000 0.0839115008712 \n"
"1100 0.104669973254 \n"
"1200 0.0776312947273 \n"
"1300 0.114788673818 \n"
"1400 0.120309025049 \n"
"1500 0.107732802629 \n"
"1600 0.070114441216 \n"
"1700 0.101728007197 \n"
"1800 0.114350572228 \n"
"1900 0.118794307113 \n"
"0 0.0669310241938 \n"
"100 0.0553173273802 \n"
"200 0.0829797014594 \n"
"300 0.0951051414013 \n"
"400 0.128303915262 \n"
"500 0.102963000536 \n"
"600 0.0910559669137 \n"
"700 0.0898950695992 \n"
"800 0.119949311018 \n"
"900 0.0603067912161 \n"
"1000 0.105748720467 \n"
"1100 0.108760476112 \n"
"1200 0.0820947736502 \n"
"1300 0.0971114039421 \n"
"1400 0.0836166366935 \n"
"1500 0.0899554267526 \n"
"1600 0.109069615602 \n"
"1700 0.0921652168036 \n"
"1800 0.0759357959032 \n"
"1900 0.100669950247"

#: ../../python/tutorial/by_examples.rst:903
msgid "We visualize embedded training images as following. You see the images from the same class embedded near each other."
msgstr "学習に用いた画像の埋め込みの結果を可視化してみましょう。同一クラスに属する画像は互いに近くに埋め込まれていることが分かります。"

#: ../../python/tutorial/by_examples.rst:906
msgid ""
"all_image = digits.images[:512, None]\n"
"all_label = digits.target[:512]"
msgstr ""
"all_image = digits.images[:512, None] \n"
"all_label = digits.target[:512]"

#: ../../python/tutorial/by_examples.rst:911
msgid ""
"x_all = nn.Variable(all_image.shape)\n"
"x_all.d = all_image"
msgstr ""
"x_all = nn.Variable(all_image.shape) \n"
"x_all.d = all_image"

#: ../../python/tutorial/by_examples.rst:916
msgid ""
"with nn.auto_forward():\n"
"    embed, _ = cnn_embed(x_all, test=True)"
msgstr ""
"with nn.auto_forward(): \n"
"    embed, _ = cnn_embed(x_all, test=True)"

#: ../../python/tutorial/by_examples.rst:921
msgid ""
"plt.figure(figsize=(16, 9))\n"
"for i in range(10):\n"
"    c = plt.cm.Set1(i / 10.)  # Maybe it doesn't work in an older version of Matplotlib where color map lies in [0, 256)\n"
"    plt.plot(embed.d[all_label == i, 0].flatten(), embed.d[\n"
"             all_label == i, 1].flatten(), '.', c=c)\n"
"plt.legend(map(str, range(10)))\n"
"plt.grid()"
msgstr ""
"plt.figure(figsize=(16, 9))\n"
"for i in range(10):\n"
"    c = plt.cm.Set1(i / 10.)  # Maybe it doesn't work in an older version of Matplotlib where color map lies in [0, 256)\n"
"    plt.plot(embed.d[all_label == i, 0].flatten(), embed.d[\n"
"             all_label == i, 1].flatten(), '.', c=c)\n"
"plt.legend(map(str, range(10)))\n"
"plt.grid()"

#: ../../python/tutorial/by_examples.rst:937
msgid "Appendix"
msgstr "付録"

#: ../../python/tutorial/by_examples.rst:940
msgid "A. Logistic Regression"
msgstr "A. ロジスティック回帰"

#: ../../python/tutorial/by_examples.rst:942
msgid ""
"Here we demonstrate how to train the simplest neural network, logistic regression (single layer perceptron). Logistic regression is a "
"linear classifier :math:`f : {\\cal R}^{D\\times 1} \\rightarrow {\\cal R}^{K\\times 1}`"
msgstr ""
"ここでは、最も簡単なニューラルネットワークであるロジスティック回帰 ( 1 層のパーセプトロン ) の学習の方法を説明します。ロジスティック回帰"
"は線形分類問題 :math:`f : {\\cal R}^{D\\times 1} \\rightarrow {\\cal R}^{K\\times 1}`"

#: ../../python/tutorial/by_examples.rst:947
msgid "\\mathbf f(\\mathbf x, \\mathbf \\Theta) = \\mathbf W \\mathbf x + \\mathbf b"
msgstr "\\mathbf f(\\mathbf x, \\mathbf \\Theta) = \\mathbf W \\mathbf x + \\mathbf b"

#: ../../python/tutorial/by_examples.rst:952
msgid ""
"where :math:`\\mathbf x \\in {\\cal R}^{D \\times 1}` is an input image flattened to a vector, :math:`t \\in \\{0, 1, \\cdots, K\\}` is "
"a target label, :math:`\\mathbf W \\in {\\cal R}^{K \\times D}` is a weight matrix, :math:`\\mathbf b \\in {\\cal R}^{K \\times 1}` is "
"a bias vector and :math:`\\mathbf \\Theta \\equiv \\left\\{\\mathbf W, \\mathbf b\\right\\}`. Loss function is defined as"
msgstr ""
"で、 :math:`\\mathbf x \\in {\\cal R}^{D \\times 1}` はベクトルへ平坦化された入力画像、 :math:`t \\in \\{0, 1, \\cdots, K\\}` は目標ラベ"
"ル、 :math:`\\mathbf W \\in {\\cal R}^{K \\times D}` は重み行列、 :math:`\\mathbf b \\in {\\cal R}^{K \\times 1}` はバイアスベクトル、 :"
"math:`\\mathbf \\Theta \\equiv \\left\\{\\mathbf W, \\mathbf b\\right\\}` です。 Loss 関数は"

#: ../../python/tutorial/by_examples.rst:959
msgid ""
"\\mathbf L(\\mathbf \\Theta, \\mathbf X) = \\frac{1}{N} \\sum_{\\mathbf x, t \\subset \\mathbf X}     -log \\left(\\left[\\sigma"
"\\left(f(\\mathbf x, \\mathbf \\Theta)\\right)\\right]_{t}\\right)"
msgstr ""
"\\mathbf L(\\mathbf \\Theta, \\mathbf X) = \\frac{1}{N} \\sum_{\\mathbf x, t \\subset \\mathbf X}     -log \\left(\\left[\\sigma"
"\\left(f(\\mathbf x, \\mathbf \\Theta)\\right)\\right]_{t}\\right)"

#: ../../python/tutorial/by_examples.rst:965
msgid ""
"where :math:`\\mathbf X \\equiv \\left\\{\\mathbf x_1, t_1, \\cdots, \\mathbf x_N, t_N\\right\\}` denotes a dataset the network trained "
"on, :math:`\\sigma(\\mathbf z)` is softmax operation defined as :math:`\\frac{\\exp(-\\mathbf z)}{\\sum_{z \\subset \\mathbf z} \\exp(-"
"z)}`, and :math:`\\left[\\mathbf z\\right]_i` denotes i-th element of :math:`\\mathbf z`."
msgstr ""
"のように定義されます。ここで、 :math:`\\mathbf X \\equiv \\left\\{\\mathbf x_1, t_1, \\cdots, \\mathbf x_N, t_N\\right\\}` はネットワー"
"クが学習に用いるデータセットを表し、 :math:`\\sigma(\\mathbf z)` は :math:`\\frac{\\exp(-\\mathbf z)}{\\sum_{z \\subset \\mathbf z} "
"\\exp(-z)}` のように定義されたソフトマックス演算で、 :math:`\\left[\\mathbf z\\right]_i` は :math:`\\mathbf z` の i 番目の要素を表してい"
"ます。"
