{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Static vs Dynamic Neural Networks in NNabla\n",
    "\n",
    "NNabla allows you to define static and dynamic neural networks. Static neural networks have a fixed layer architecture, i.e., a static computation graph. In contrast, dynamic neural networks use a dynamic computation graph, e.g., randomly dropping layers for each minibatch.\n",
    "\n",
    "This tutorial compares both computation graphs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install nnabla-ext-cuda114\n",
    "!git clone https://github.com/sony/nnabla.git\n",
    "%cd nnabla/tutorial"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "import nnabla as nn\n",
    "import nnabla.functions as F\n",
    "import nnabla.parametric_functions as PF\n",
    "import nnabla.solvers as S\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "GPU = 0  # ID of GPU that we will use\n",
    "batch_size = 64  # Reduce to fit your device memory"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset loading\n",
    "\n",
    "We will first setup the digits dataset from scikit-learn:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tiny_digits import *\n",
    "\n",
    "digits = load_digits()\n",
    "data = data_iterator_tiny_digits(digits, batch_size=batch_size, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each sample in this dataset is a grayscale image of size 8x8 and belongs to one of the ten classes `0`, `1`, ..., `9`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img, label = data.next()\n",
    "print(img.shape, label.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network definition\n",
    "\n",
    "As an example, we define a (unnecessarily) deep CNN:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def cnn(x):\n",
    "    \"\"\"Unnecessarily Deep CNN.\n",
    "    \n",
    "    Args:\n",
    "        x : Variable, shape (B, 1, 8, 8)\n",
    "        \n",
    "    Returns:\n",
    "        y : Variable, shape (B, 10)\n",
    "    \"\"\"\n",
    "    with nn.parameter_scope(\"cnn\"):  # Parameter scope can be nested\n",
    "        with nn.parameter_scope(\"conv1\"):\n",
    "            h = F.tanh(PF.batch_normalization(\n",
    "                PF.convolution(x, 64, (3, 3), pad=(1, 1))))\n",
    "        for i in range(10):  # unnecessarily deep\n",
    "            with nn.parameter_scope(\"conv{}\".format(i + 2)):\n",
    "                h = F.tanh(PF.batch_normalization(\n",
    "                    PF.convolution(h, 128, (3, 3), pad=(1, 1))))\n",
    "        with nn.parameter_scope(\"conv_last\"):\n",
    "            h = F.tanh(PF.batch_normalization(\n",
    "                PF.convolution(h, 512, (3, 3), pad=(1, 1))))\n",
    "            h = F.average_pooling(h, (2, 2))\n",
    "        with nn.parameter_scope(\"fc\"):\n",
    "            h = F.tanh(PF.affine(h, 1024))\n",
    "        with nn.parameter_scope(\"classifier\"):\n",
    "            y = PF.affine(h, 10)\n",
    "    return y"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Static computation graph\n",
    "\n",
    "First, we will look at the case of a static computation graph where the neural network does not change during training.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from nnabla.ext_utils import get_extension_context\n",
    "\n",
    "# setup cuda extension\n",
    "ctx_cuda = get_extension_context('cudnn', device_id=GPU)  # replace 'cudnn' by 'cpu' if you want to run the example on the CPU\n",
    "nn.set_default_context(ctx_cuda)\n",
    "\n",
    "# create variables for network input and label\n",
    "x = nn.Variable(img.shape)\n",
    "t = nn.Variable(label.shape)\n",
    "\n",
    "# create network\n",
    "static_y = cnn(x)\n",
    "static_y.persistent = True\n",
    "\n",
    "# define loss function for training\n",
    "static_l = F.mean(F.softmax_cross_entropy(static_y, t))"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setup solver for training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "solver = S.Adam(alpha=1e-3)\n",
    "solver.set_parameters(nn.get_parameters())"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create data iterator"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loss = []\n",
    "def epoch_end_callback(epoch):\n",
    "    global loss\n",
    "    print(\"[\", epoch, np.mean(loss), itr, \"]\", end='\\n')\n",
    "    loss = []\n",
    "\n",
    "data = data_iterator_tiny_digits(digits, batch_size=batch_size, shuffle=True)\n",
    "data.register_epoch_end_callback(epoch_end_callback)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform training iterations and output training loss:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    itr = 0\n",
    "    while data.epoch == epoch:\n",
    "        x.d, t.d = data.next()\n",
    "        static_l.forward(clear_no_need_grad=True)\n",
    "        solver.zero_grad()\n",
    "        static_l.backward(clear_buffer=True)\n",
    "        solver.update()\n",
    "        loss.append(static_l.d.copy())\n",
    "        itr += 1\n",
    "print('')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dynamic computation graph\n",
    "\n",
    "Now, we will use a dynamic computation graph, where the neural network is setup each time we want to do a forward/backward pass through it. This allows us to, e.g., randomly dropout layers or to have network architectures that depend on input data. In this example, we will use for simplicity the same neural network structure and only dynamically create it. For example, adding a `if np.random.rand() > dropout_probability:` into `cnn()` allows to dropout layers.\n",
    "\n",
    "First, we setup the solver and the data iterator for the training:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nn.clear_parameters()\n",
    "solver = S.Adam(alpha=1e-3)\n",
    "solver.set_parameters(nn.get_parameters())\n",
    "\n",
    "loss = []\n",
    "def epoch_end_callback(epoch):\n",
    "    global loss\n",
    "    print(\"[\", epoch, np.mean(loss), itr, \"]\", end='\\n')\n",
    "    loss = []\n",
    "data = data_iterator_tiny_digits(digits, batch_size=batch_size, shuffle=True)\n",
    "data.register_epoch_end_callback(epoch_end_callback)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    itr = 0\n",
    "    while data.epoch == epoch:\n",
    "        x.d, t.d = data.next()\n",
    "        with nn.auto_forward():\n",
    "            dynamic_y = cnn(x)\n",
    "            dynamic_l = F.mean(F.softmax_cross_entropy(dynamic_y, t))\n",
    "        solver.set_parameters(nn.get_parameters(), reset=False, retain_state=True) # this can be done dynamically\n",
    "        solver.zero_grad()\n",
    "        dynamic_l.backward(clear_buffer=True)\n",
    "        solver.update()\n",
    "        loss.append(dynamic_l.d.copy())\n",
    "        itr += 1\n",
    "print('')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comparing the two processing times, we can observe that both schemes (\"static\" and \"dynamic\") takes the same execution time, i.e., although we created the computation graph dynamically, we did not loose performance."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "accelerator": "GPU",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}